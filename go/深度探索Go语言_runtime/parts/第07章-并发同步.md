第 7 章
同步
在一开始接触多线程编程的时候，我们就被告知同步有多么重要，那个经典的银行取款的例子也已
经听过了很多遍。之所以称为同步，就是因为存在并发，不过大多数对于并发同步的讲解都太上层
了。本章通过对编译、执行及一些硬件特性的探索，进一步加深大家对同步的理解，希望能够帮助
大家写出更健壮的程序。
7.1 Happens Before
在多线程的环境中，多个线程或协程同时操作内存中的共享变量，如果不加限制，就会出现出乎意
料的结果。想保证结果正确，就需要在时序上让来自不同线程的访问串行化，彼此之间不出现重
叠。线程对变量的操作一般只有Load和Store两种，就是我们俗称的读和写。Happens Before也可以
认为是一种串行化描述或要求，目的是保证某个线程对变量的写操作，能够被其他的线程正确地读
到。

按照字面含义，你可能会认为，如果事件e2在时序上于事件e1结束后发生，就可以说事件e1
happens before e2了。按照一般常识应该是这样的，在我们介绍内存乱序之前暂时可以这样理解，
事实上这对于多核环境下的内存读写操作来讲是不够的。

如果e1 happens before e2，则可以说成e2 happens after e1。若要保证对变量v的某个读操作r，能够
读取到某个写操作w写入v的值，必须同时满足以下条件：

（ 1 ）w happens before r。

（ 2 ）没有其他针对v的写操作happens after w且before r。

如果e1既不满足happens before e2，又不满足happens after e2，就认为e1与e2之间存在并发，如图7-1
所示。单个线程或协程内部访问某个变量是不存在并发的，默认能满足happens before条件，因此
某个读操作总是能读到之前最近一次写操作写入的值，但是在多个线程或协程之间就不一样了，因
为存在并发的原因，必须通过一些同步机制实现串行化，以确立happens before条件。

7.1.1 并发
我们知道现代操作系统是基于时间片算法来调度线程的，goroutine也实现了基于时间片的抢占式调
度。当线程的时间片用完时，可能会在任意两条机器指令间被打断。假设线程t1即将执行一个针对
变量v的写操作w，而线程t2即将执行一个针对变量v的读操作r，我们想要让r读取w写入的值，也就
是要让w happens before r。暂定我们的执行环境只有一个CPU内核，所以任一时刻t1和t2只能有一
个在执行。即使这样也依然有问题，t1可能在执行w操作之前就被打断了，然后t2执行了r操作。如
果不使用一些同步机制，我们无法保证t2的r操作执行时，t1的w操作已经执行完了。最常用的同步
工具就是锁，但是针对某些特定场景，我们不用锁也可以让程序得到正确的结果。

图7-1 多线程并发事件示意图
例如经典的生产者、消费者场景，有两个线程分别是生产者和消费者，两者之间通过共享变量来传
递数据。为了让程序能够像预期那样运行，消费者线程必须在生产者线程完成共享变量的写操作之
后才去读，生产者线程也必须在消费者线程完成读取之后才能再次将新的值写入共享变量，两者需
要一直交替地执行。
可以通过引入另外一个变量实现这个目的，接下来就尝试用Go语言实现，例如原本的共享变量是
int类型的变量data，我们再引入一个bool型变量ok，用来表示data的所有权，代码如下：

当变量ok为false时，data的所有权归生产者所有，生产者首先为data赋值，完成之后再把ok设置为
true，从而把data的所有权传递给了消费者，代码如下：

当变量ok为true时，data的所有权归消费者所有，消费者读取完data的值后，再把ok的值设置为
false，也就是把data的所有权又回传给了生产者，代码如下：

如果编译器生成的指令与源码中语句的顺序严格一致，上述生产者协程和消费者协程在单核CPU上
并发执行是可以保证结果正确的。一旦编译器对生成指令的顺序进行优化调整，或者程序在多核
CPU上执行，就不能保证结果正确了，具体原因接下来会逐步分析。
在单核CPU上分时交替运行的多个线程，可以认为是最经典的并发场景。宏观上看起来同时在运行
的多个线程，微观上是以极短时间片交替运行在同一个CPU上的。在多核CPU出现以前，并发指的
就是这种情况，但是在多核CPU出现以后，并发就不像以前那么简单了，不仅是微观上的分时运
行，还包含了并行的情况。
7.1.2 并行
抽象地解释并发，指的是多个事件在宏观上是同时发生的，但是并不一定要在同一时刻发生，而并
行就不一样了，从微观角度来看，并行的两个事件至少有某一时刻是同时发生的，所以在单核CPU
上的多线程只存在并发，不存在并行。只有在多核CPU上，线程才有可能并行执行。
针对7.1.1节中的生产消费示例，我们说过，如果编译器不调整指令顺序，并且在单核CPU上执行程
序，就可以保证结果的正确性。如果在多核CPU上运行，就不能保证结果正确了，这里不能简单地
认为是受并行的影响，根本原因是运行在不同CPU核心上的线程间可能会存在内存乱序，从现象来
看就像是CPU在运行阶段调整了某些指令的顺序一样。我们将在7.2节中对内存乱序展开更深入和
全面的探索。
7.2 内存乱序
一般来讲，我们认为代码会按照编写的顺序来执行，也就是逐语句、逐行地按顺序执行，然而事实
并非如此，编译器有可能对指令顺序进行调整，处理器普遍具有乱序执行的特性，目的都是为了更
优的性能。操作内存的指令可以分成Load和Store两类，也就是按读和写划分。编译器和CPU都会
考虑指令间的依赖关系，在不会改变当前线程行为的前提下进行顺序调整，因此在单个线程内依然
是逻辑有序的，语句间原本满足的happens before条件不会被破坏，但这种有序性只是在单个线程
内，并不会保证线程间的有序性。

程序中的共享变量都位于内存中，指令顺序的变化会让多个线程同时读写内存中的共享变量时产生
意想不到的结果。这种因指令乱序造成的内存操作顺序与预期不一致的问题，就是所谓的内存乱
序。

7.2.1 编译期乱序
所谓的编译期乱序，指的是编译器对最终生成的机器指令进行了顺序调整，一般是出于性能优化的
目的。造成的影响就是，机器指令的顺序与源代码中语句的顺序并不严格一致。这种乱序在C++中
比较常见，尤其是在编译器的优化级别比较高的时候。
还是以生产者消费者为例，这次改成用C++实现。源码中有个整型共享变量data，它会被一对生产
者、消费者线程操作，为了协调这两个线程，我们又加了一个bool型变量ok，代码如下：

生产者和消费者分别运行在两个线程中，都循环执行处理逻辑。生产者每次循环开始时会先检查ok
的值，一直等到ok为false，也就表示data中没有数据，此时生产者就先为data赋值，再把ok设为
true，表示data中的数据已经就绪了，代码如下：

消费者每次循环开始时也会先检查ok的值，一直等到ok为true后才去消费data中的数据，完成后再
把ok的值设为false，这样生产者就可以生产新的数据了，代码如下：

按照预期，这个程序应该能够正常运行，但是有时候结果可能会出乎意料，原因就是刚刚讲过的编
译乱序问题。按照之前的设计，用ok来表示data当前的状态，生产者和消费者相互传递data的所有

权，这非常依赖data和ok的内存访问顺序。生产者和消费者都要先检查ok的值，在条件允许，也就
是获取到所有权的情况下，先操作data，后为ok赋值。这个顺序是不能颠倒的，一旦改变了ok的
值，就把data的所有权交给了对方。

编译器并不知道这些，它只要保证单个线程的行为不被改变就可以了。经过编译优化之后，生产者
可能变成先把ok设置为true，再为data赋值，消费者也可能先把ok设置为false，再读取data的值，所
以运行结果就会出现错误。

那么如何解决这种编译阶段的乱序问题呢？最常用的方法就是使用compiler barrier，俗称编译屏
障。编译屏障会阻止编译器跨屏障移动指令，但是仍然可以在屏障的两侧分别移动。在GCC中，
常用的编译屏障就是在两条语句之间嵌入一个空的汇编语句块，代码如下：

上面的示例加上编译屏障后，应该能够在x86平台上正常运行了，但是依然无法保证能够在其他平
台上如预期地运行，原因就是CPU在执行期间也可能会对指令的顺序进行调整，也就是我们接下来
要探索的执行期乱序。

7.2.2 执行期乱序
笔者已经不止一次地提到过，CPU可能在执行期间对指令顺序进行调整，也就是这里所谓的执行期
乱序。在进行枯燥的分析之前，先用一段代码来让大家亲自见证执行期乱序，这样更有助于后续内
容的理解。示例代码使用Go语言实现，平台是amd64，代码如下：

代码中一共有 3 个协程， 4 个int类型的共享变量， 3 个协程都会循环 100 万次， 3 个channel用于同步每
次循环。循环开始时先由主协程将x、y清零，然后通过切片s中的两个channel让其他两个协程开始
运行。协程一在每轮循环中先把 1 赋值给x，再把y赋值给b。协程二在每轮循环中先把 1 赋值给y，再
把x赋值给a。f用来保证在每轮循环中都等到两个协程完成赋值操作后，主协程才去检测a和b的
值，当两者同时为 0 时会打印出当前循环的次数。

从源码角度来看，无论如何a和b都不应该同时等于 0 。如果协程一完成赋值后协程二才开始执行，
结果就是a＝ 1 而b＝ 0 ，反过来就是a等于 0 而b等于 1 。如果两个协程的赋值语句并行执行，则结果就
是a和b都等于 1 ，然而实际运行时会发现大量打印输出，根本原因就是出现了执行期乱序。注意，
执行期乱序要在并行环境下才能体现出来，单个CPU核心自己是不会体现出乱序的。Go程序可以
使用GOMAXPROCS环境变量来控制P的数量，针对上述示例代码，将GOMAXPROCS设置为 1 即使
在多核心CPU上也不会出现乱序。

协程一和协程二中的两条赋值语句形式相似，对应到x86汇编就是三条内存操作指令，按照顺序及
分类分别是Store、Load、Store，如图7-2所示。

出现的乱序问题是由前两条指令造成的，称为Store-Load乱序，这也是当前x86架构CPU上能够观察
到的唯一一种乱序。Store和Load分别操作的是不同的内存地址，从现象来看就像是先执行了Load
而后执行了Store。

为什么会出现Store-Load乱序呢？我们知道现在的CPU普遍带有多级指令和数据缓存，指令执行系
统也是流水线式的，可以让多条指令同时在流水线上执行。一般的内存属于write-back cacheable内
存，简称WB内存。对于WB内存而言，Store和Load指令并不是直接操作内存中的数据的，而是先
把指定的内存单元填充到高速缓存中，然后读写高速缓存中的数据。

图7-2 协程一和协程二的赋值语句对应的汇编指令
Load指令的大致流程是，先尝试从高速缓存中读取，如果缓存命中，则读操作就完成了，如图7-
3 （a）所示。如果缓存未命中，则先填充对应的Cache Line，然后从Cache Line中读取，如图7-
3 （b）所示。

图7-3 Load指令的执行流程

Store指令的大致流程类似，先尝试写高速缓存，如果缓存命中，则写操作就完成了。如果缓存未
命中，则先填充对应的Cache Line，然后写到Cache Line中，如图7-4所示。

图7-4 Store指令执行流程

可能有些读者会对Store操作写之前要先填充Cache Line感到疑惑，这是因为高速缓存和内存之间的
数据传输不是以字节为单位的，最小单位就是一个Cache Line。Cache Line大小因处理器的架构而
异，常见的大小有 32 、 64 及 128 字节等。

在多核心的CPU上，Store操作会变得更复杂一些。每个CPU核心都拥有自己的高速缓存，例如x86
的L1 Cache。写操作会修改当前核心的高速缓存，被修改的数据可能存在于多个核心的高速缓存
中，CPU需要保证各个核心间的缓存一致性。目前主流的缓存一致性协议是MESI协议，MESI这个
名字取自缓存单元可能的 4 种状态，分别是已修改的Modified，独占的Exclusive，共享的Shared和无
效的Invalid。

如图7-5所示，当一个CPU核心要对自身高速缓存的某个单元进行修改时，它需要先通知其他CPU
核心把各自高速缓存中对应的单元置为Invalid，再把自己的这个单元置为Exclusive，然后就可以进
行修改了。

图7-5 一个CPU核心修改高速缓存数据单元的过程
这个过程涉及多核间的内部通信，是一个相对较慢的过程，为了避免当前核心因为等待而阻塞，
CPU在设计上又引入了Store Buffer。当前核心向其他核心发出通知以后，可以先把要写的值放在
Store Buffer中，然后继续执行后面的指令，等到其他核心完成响应以后，当前核心再把Store Buffer
中的值合并到高速缓存中，如图7-6所示。

图7-6 引入Store Buffer后CPU修改高速缓存数据单元的过程

虽然高速缓存会保证多核一致性，但是Store Buffer却是各个核心私有的，因此对其他核心不可见。
在Store-Load乱序中，从微观时序上，Load指令可能是在另一个线程的Store之后执行，但此时多核
间通信尚未完成，对应的缓存单元还没有被置为Invalid，Store Buffer也没有被合并到高速缓存中，
所以Load读到的是修改前的值。

如图7-7所示，如果协程一执行了Store命令，x的新值只是写入CPU1的Store Buffer，尚未合并到高
速缓存，则此时协程二执行Load指令获得的x就是修改前的旧值 0 ，而不是 1 。同样地，协程二修改
y的值也可能只写入了CPU2的Store Buffer，所以协程一执行Load指令加载的y的值就是旧值 0 。

而当协程一执行最后一条Store指令时，b就被赋值为 0 。同样地，协程二会将a赋值为 0 。即使Store
Buffer合并到高速缓存，x和y都被修改为新值，也已经晚了，如图7-8所示。

图7-7 写入Store Buffer后合并到高速缓存前Load数据

图7-8 合并到高速缓存后的数据状态
我们通过代码示例见证了x86的Store-Load乱序，Intel开发者手册上说x86只会出现这一种乱序。抛
开固定的平台架构，理论上可能出现的乱序有 4 种：

（ 1 ）Load-Load，相邻的两条Load指令，后面的比前面的先读到数据。

（ 2 ）Load-Store，Load指令在前，Store指令在后，但是Store操作先变成全局可见，Load指令在此
之后才读到数据。

（ 3 ）Store-Load，Store指令在前，Load指令在后，但是Load指令先读到了数据，Store操作在此之
后才变成全局可见。这个我们已经在x86平台见证过了。

（ 4 ）Store-Store，相邻的两条Store指令，后面的比前面的先变成全局可见。

所谓的全局可见，指的是在多核CPU上对所有核心可见。因为笔者手边只有amd64架构的计算机，
暂时无法验证其他几种乱序，有条件的读者可以在其他的架构上尝试一下。例如通过以下示例应该
可以发现Store-Store乱序，代码如下：

7.2.3 内存排序指令
执行期乱序会给结果带来很大的不确定性，这对于应用程序来讲是不能接受的，完全按照指令顺序
执行又会使性能变差。为了解决这一问题，CPU提供了内存排序指令，应用程序在必要的时候能够
通过这些指令来避免发生乱序。以目前的Intel x86处理器为例，提供了LFENCE、SFENCE和

MFENCE这 3 条内存排序指令，接下来我们就逐一分析它们的作用。
LFENCE是Load Fence的缩写，Fence翻译成中文是栅栏，可以认为起到分隔的作用，它会对当前核
心上LFENCE之前的所有Load类指令进行序列化操作。具体来讲，针对当前CPU核心，LFENCE会
在之前的所有指令都执行完后才开始执行，并且在LFENCE执行完之前，不会有后续的指令开始执
行。特别是LFENCE之前的Load指令，一定会在LFENCE执行完成之前从内存接收到数据。
LFENCE不会针对Store指令，Store指令之后的LFENCE可能会在Store写入的数据变成全局可见前执
行完成。LFENCE之后的指令可以提前被从内存中加载，但是在LFENCE执行完之前它们不会被执
行，即使是推测性的。

以上主要是Intel开发者手册对LFENCE的解释，它原本被设计用来阻止Load-Load乱序。让所有后
续的指令在之前的指令执行完后才开始执行，这是Intel对功能的一个扩展，因此理论上它应该也能
阻止Load-Store乱序。考虑到目前的x86 CPU不会出现这两种乱序，所以编程语言中暂时没有用到
LFENCE指令进行多核同步，未来也许会用到。Go的runtime中用到了LFENCE的扩展功能来对
RDTSC进行序列化，但是这并不属于同步的范畴。

SFENCE是Store Fence的缩写，它能够分隔两侧的Store指令，保证之前的Store操作一定会在之后的
Store操作变成全局可见前先变成全局可见。结合7.2.2节的高速缓存和Store Buffer，笔者猜测
SFENCE会影响到Store Buffer合并到高速缓存的顺序。

根据上述解释，SFENCE应该主要用来应对Store-Store乱序，由于现阶段的x86 CPU也不会出现这种
乱序，所以编程语言暂时也未用到它进行多核同步。

MFENCE是Memory Fence的缩写，它会对之前所有的Load和Store指令进行序列化操作，这个序列
化会保证MFENCE之前的所有Load和Store操作会在之后的任何Load和Store操作前先变成全局可
见，所以上述 3 条指令中，只有MFENCE能够阻止Store-Load乱序。

我们对之前的示例代码稍做修改，尝试使用MFENCE指令来阻止Store-Load乱序，新的示例中用到
了汇编语言，所以需要两个源码文件。首先是汇编代码文件fence_amd64.s，代码如下：

接下来是修改过的Go代码，被放置在fence.go文件中，跟之前会发生乱序的代码只有一点不同，就
是在Store和Load之间插入了MFENCE指令，代码如下：

编译执行上述代码，会发现之前的Store-Load乱序不见了，程序不会有任何打印输出。如果将
MFENCE指令换成LFENCE或SFENCE，就无法达到同样的目的了，感兴趣的读者可以自己尝试一
下。

通过内存排序指令解决了执行期乱序造成的问题，但是这并不足以解决并发场景下的同步问题。要
想结合代码逻辑轻松地实现多线程同步，就要用到专门的工具，这就是7.3节要介绍的锁。

7.3 常见的锁
本书的测试代码都比较简单，实际编程时的业务逻辑往往要复杂得多，需要同步保护的临界区中通
常会有数十数百条指令，甚至更多。锁需要将所有线程（或协程）对临界区的访问进行串行化处
理，需要同时保证两点要求：
（ 1 ）同时只能有一个线程获得锁，持有锁才能进入临界区。
（ 2 ）当线程离开临界区释放锁后，线程在临界区内做的所有操作都要全局可见。
本节会介绍几种在编程中常见的锁，并简单分析它们各自的实现原理，在此过程中需留意各种锁是
如何保证以上两点要求的。
7.3.1 原子指令
软件层面的锁通常被实现为内存中的一个共享变量，加锁的过程至少需要 3 个步骤，按顺序依次是
Load、Compare和Store。Load操作从内存中读取锁的最新状态，Compare操作用于检测是否处于未
加锁状态，如果未加锁就通过Store操作进行修改，以便实现加锁。如果Compare发现已经处于加锁
状态了，就不能执行后续的Store操作了。

如果用一般的x86汇编指令实现Load-Compare-Store操作，至少需要三条指令，例如CMP、JNE和
MOV。CMP可以接收一个内存地址操作数，所以实质上包含了Load和Compare两步，JNE作为
Compare的一部分用于实现条件跳转，MOV指令用来向指定内存地址写入数据，也就是Store操
作，但是这样实现会有一个问题，我们知道线程用完时间片之后会被打断，假如线程a执行完CMP
指令后发现未加锁，但是在执行MOV之前被打断了，然后线程b开始执行并获得了锁，接下来线程
b在临界区中被打断，线程a恢复执行后也获得了锁，这样一来就会出现错误，如图7-9所示。

所以我们需要在一条指令中完成整个Load-Compare-Store操作，必须从硬件层面提供支持，例如x86
就提供了CMPXCHG指令。

图7-9 同步问题
CMPXCHG是Compare and Exchange的缩写，该指令有两个操作数，用于实现锁的时候，第一操作
数通常是个内存地址，也称为目的操作数，第二操作数是个通用寄存器。CMPXCHG会将AX寄存
器和第一操作数进行比较，如果相等就把第二操作数复制到目的操作数中，若不相等就把目的操作
数复制到AX寄存器中。基于这个指令实现锁，一条指令是不会在中间被打断的，所以就解决了之
前的问题。

在单核环境下，任何能够通过一条指令完成的操作都可以称为原子操作，但是这也只适用于单核场
景，在多核环境下，运行在不同CPU核心上的线程可能会并行加锁，不同核心同时执行CMPXCHG
又会造成多个线程同时获得锁。如何解决这个问题呢？一种思路是，在当前核心执行CMPXCHG
时，阻止其他核心执行CMPXCHG，x86汇编中的LOCK前缀用于实现这一目的。

LOCK前缀能够应用于部分内存操作指令，最简单的解释就是LOCK前缀会让当前CPU核心在当前
指令执行期间独占总线，这样其他的CPU核心就不能同时操作内存了。事实上，只有对于不在高速
缓存中的数据才会这样，对于高速缓存中的数据，LOCK前缀会通过MESI协议处理多核间缓存一
致性。不管怎么说，加上LOCK前缀的CMPXCHG就无懈可击了。在多核环境下，这种带有LOCK
前缀的指令也被称为原子指令。

至此，针对锁的两点要求，其中第 1 个可以通过原子指令实现了。那么如何做到第二点要求呢？就
是释放锁之后，临界区内所有的操作要全局可见。事实上，锁本身的状态变化就必须是全局可见
的，而且必须很及时，以保证高性能，因此，在x86 CPU上，LOCK前缀同时具有内存排序的作
用，相当于在应用LOCK前缀的指令之后紧接着执行了一条MFENCE指令。综上所述，原子指令既
能保证只允许一个线程进入临界区，又具有内存排序的作用，能够保证在锁的状态发生变化时，临
界区中所有的修改随锁的状态一起变成全局可见。

7.3.2 自旋锁
自旋锁得以实现的基础是原子性的CAS操作，CAS即Compare And Swap，在x86平台上对应带有
LOCK前缀的CMPXCHG指令。之所以称作自旋锁，是因为它会一直循环尝试CAS操作直到成功，
看起来就像是一直在自旋等待。

接下来我们就尝试一下用汇编语言基于CMPXCHG指令实现一把自旋锁，首先在Go语言中基于
int32创建一个自定义类型Spin，并为它实现Lock（）方法和Unlock（）方法，代码如下：

实际的加锁和解锁操作在lock（）和unlock（）这两个函数中实现，Go代码中只包含了这两个函数
的原型声明，这两个函数是用汇编语言实现的，具体代码在spin_amd64.s文件中，代码如下：

lock（）函数把锁的地址放在了BX寄存器中，把用来比较的旧值old放到了DX寄存器中，把要写入
的新值new放到了CX寄存器中。从标签again处开始是一个循环，每次循环开始前，把DX寄存器的
值复制给AX寄存器，因为CMPXCHG隐含使用AX寄存器中的值作为比较用的旧值，并且可能会修
改AX寄存器，所以每次循环需要重新赋值，这个循环不断尝试通过CMPXCHG进行加锁，成功后
会通过JE指令跳出循环。因为Go的汇编风格有点类似于AT&T汇编，操作数书写顺序与Intel汇编相
反，所以CMPXCHG的两个操作数中BX出现在CX右边。能够通过JE跳出循环，这是因为CMP操作
会影响标志寄存器。

unlock（）函数通过XCHG指令将锁清零，实现了解锁操作。细心的读者可能会注意到这里没有
LOCK前缀，根据Intel开发者手册所讲，XCHG指令隐含了LOCK前缀，所以代码中不用写，依然
能够起到独占总线和内存排序的作用。

事实上，atomic包中的CompareAndSwapInt32（）函数和StoreInt32（）函数是基于CMPXCHG和
XCHG这两条汇编指令实现的，所以上述的自旋锁可以改成完全用Go实现，代码如下：

这样一来，我们确实实现了自旋锁，但是这跟生产环境中实际使用的自旋锁比起来还是有些差距。
在锁竞争比较激烈的场景下，这种自旋会造成CPU使用率很高，所以还要进行优化。x86专门为此
提供了PAUSE指令，它一方面能够提示处理器当前正处于自旋循环中，从而在退出循环的时候避
免因检测到内存乱序而造成性能损失。另一方面，PAUSE能够大幅度减小自旋造成的CPU功率消

耗，从而达到节能和减少发热的效果。
可以把PAUSE指令加入我们汇编版本的lock（）函数实现中，修改后的代码如下：

也可以把PAUSE指令单独放在一个函数中，这样就能够跟atomic包中的函数结合使用了，代码如
下：

然后就能对Go代码实现的自旋锁进行优化了，代码如下：

自旋锁的优点是比较轻量，不过它对适用的场景也是有要求的。首先，在单核心的环境下不适合使
用自旋锁，因为单核系统上任一时刻只能有一个线程在运行，当前线程一直在自旋等待，而持有锁
的线程得不到运行，锁就不可能被释放，等也是白等，纯属浪费CPU资源。这种情况下及时切换到
其他可运行的线程会更高效一些，因此在单核环境下更适合用调度器对象。其次，即使是在多核环
境下，也要考虑平均持有锁的时间，以及程序的并发程度等因素。在持有锁的时间占比很小，并且
活跃线程数接近CPU核心数量时，自旋锁比较高效，也就是自旋的代价小于线程切换的代价。其他
情况就不一定了，要结合实际场景分析再加上充分的测试。
7.3.3 调度器对象
笔者使用调度器对象这个名字，主要是受Windows NT内核的影响。更通俗地讲，应该说是操作系
统提供的线程间同步原语，一般以一组系统调用的形式存在。例如Win32的Event，以及Linux的
futex等。基于这些同步原语，可以实现锁及更复杂的同步工具。

这些调度器对象与自旋锁的不同主要是有一个等待队列。当线程获取锁失败时不会一直在那里自
旋，而是挂起后进入等待队列中等待，然后系统调度器会切换到下一个可运行的线程。等到持有锁
的线程释放锁的时候，会按照一定的算法从等待队列中取出一个线程并唤醒它，被唤醒的线程会获
得所有权，然后继续执行。这些同步原语是由内核提供的，直接与系统的调度器交互，能够挂起和
唤醒线程，这一点是自旋锁做不到的。等待队列可以实现支持FIFO、FILO，甚至支持某种优先级
策略，但是也正是由于是在内核中实现的，所以应用程序需要以系统调用的方式来使用它，这就造
成了一定的开销。在获取锁失败的情况下还会发生线程切换，进一步增大开销。调度器对象和自旋
锁各自有适用的场景，具体如何选用还要结合具体场景来分析。
7.3.4 优化的锁
通过7.3.2节和7.3.3节，我们大致了解了自旋锁与调度器对象。前者主要适用于多核环境，并且持有
锁的时间占比较小的情况。这种情况下，往往在几次自旋之后就能获得锁，比起发生一次线程切换
的代价要小得多。后者主要适用于加锁失败就要挂起线程的场景，例如单核环境，或者持有锁的时
间占比较大的情况，而在实际的业务逻辑中，持有锁的时间往往不是很确定，有可能较短也有可能
较长，我们不好一概用一种策略进行处理，如果将两者结合，或许会有不错的效果。
将自旋锁和调度器对象结合，理论上就可以得到一把优化的锁了。加锁时首先经过自旋锁，但是需
限制最大自旋次数，如果在有限次数内加锁成功也就成功了，否则就进一步通过调度器对象将当前
线程挂起。等到持有锁的线程释放锁的时候，会通过调度器对象将挂起的线程唤醒。这样就结合了
二者的优点，既避免了加锁失败立即挂起线程造成过多的上下文切换，又避免了无限制地自旋而空
耗CPU，这也是如今主流的锁实现思路。
7.4 Go 语言的同步
7.1～7.3节用了很大的篇幅讲解了与同步相关的一些理论基础，本节就回归到Go语言上来，结合
runtime源码，分析一下与同步相关的组件的实现原理。

7.4.1 runtime.mutex

在Go 1.14版本的runtime中，mutex的定义代码如下：

在Go 1.15及以后的版本中为了支持静态的Lock Rank而添加了lockRankStruct，这里暂时不需要关
心。

runtime.mutex被runtime自身的代码使用，它是针对线程而设计的，不适用于协程。它本质上就是一
个结合了自旋锁和调度器对象的优化过的锁，自旋锁部分没有什么特殊的，调度器对象部分在不同
平台上需要使用不同的系统调用。在Linux上是基于futex实现的，该实现中把mutex.key作为一个
uint32来使用，并且为其定义了 3 种状态，对应的 3 个常量的定义代码如下：

unlocked表示当前处于未加锁状态，locked则表示已加锁状态，sleeping比较特殊一点，表示当前有
线程因未能获得锁而通过futex睡眠等待。加锁函数的源代码如下：

首先通过atomic.Xchg（）函数将l.key替换成mutex_locked，然后判断原始值v，如果等于
mutex_unlocked，就说明原本处于未加锁状态，而我们现在已经通过原子操作加了锁，这样就可以
返回了。

既然v不等于mutex_unlocked，那就只能是mutex_locked和mutex_sleeping二者之一了，先把它的值

暂存在wait中。接下来根据处理器核心数ncpu是否大于 1 来决定是否需要自旋，因为在单核心系统
上自旋是没有意义的。active_spin是个值为 4 的常量，表示主动自旋 4 次。

接下来就是尝试加锁的大循环了，大循环内部先经过两个小循环。第 1 个小循环是主动自旋的循
环，它会循环spin次，也就是单核环境下循环 0 次，多核环境下循环 4 次。每次尝试之后都会通过
procyield（）函数来稍微拖延一下时间，procyield（）函数是汇编语言实现的函数，代码如下：

实际上就是循环执行PAUSE指令。active_spin_count是个值为 30 的常量，所以就是循环执行 30 次
PAUSE。

第 2 个小循环是个被动自旋循环。passive_spin是个值为 1 的常量，所以只会循环一次。之所以称为
被动自旋，是因为它调用了osyield（）函数来等待，这也是它与主动自旋的唯一一点不同。
osyield（）函数也是个用汇编语言实现的函数，它通过执行系统调用来切换至其他线程，代码如
下：

上述两个循环的主要工作都是检测锁是否已经被释放了，假如有一个锁l，线程b尝试加锁，进入加
锁的大循环，经过主动自旋和被动自旋两个小循环，如果自旋过程中发现锁被释放了，并且锁的原
始状态为mutex_locked，则表示在b加锁之前有其他线程持有锁，却没有线程在等待它，所以就将l
置为mutex_locked。若是锁的原始状态为mutex_sleeping，则表示已经有其他线程在等待这个锁了，
那么现在即使线程b获得了锁，也应该将锁置为mutex_sleeping。

总而言之，只要自旋过程中加锁成功，就得将锁置为其原始值，也就是源码中保存到wait中的状
态，如图7-10所示。

图7-10 自旋过程中获得锁
这里需要注意一下，如果持有锁的线程在释放的时候发现锁的状态为mutex_sleeping，就会通过
futex唤醒睡眠等待的线程。假如线程a持有锁l，线程b在睡眠等待这个锁，接下来线程c尝试加锁，
它首先通过atomic.Xchg（）函数把锁的状态替换为mutex_locked，然后进入自旋。恰巧，在线程c
自旋过程中线程a要释放锁，但此时锁的状态为mutex_locked，释放锁时不会去唤醒等待的线程，
而线程c却会获得锁，不过会将锁恢复为mutex_sleeping状态。这一过程中锁l的状态变化如图7-11所
示。

图7-11 一次插队过程中锁的状态变化
整个过程下来，相当于线程c跳过了futex排队，直接从锁的上一个持有者线程a那里接收了所有权，
通过futex唤醒睡眠线程的操作被推后了，但是并没有被忘记。这样相当于发生了一次插队，但是
避免了一次线程切换，从整体上来看会提升性能。

然而，若是经过上述两个自旋循环都没能获得锁，就可以通过atomic.Xchg（）函数把l.key替换为
mutex_sleeping，因为当前线程准备要去睡眠等待了，但是仍要在真正去睡眠之前，检查一下锁是
否被释放了，若已经释放，则当前线程仍可以加锁成功，然后就可以直接返回了。不过直接将
mutex_sleeping状态保留在锁中，可能会有点小问题。

因为，可能锁的原始状态为mutex_locked，并没有线程在futex上睡眠等待这个锁，因而在释放锁的
时候可能会有多余的唤醒操作。不过没有关系，这样的小问题会被忽略，只要保证不丢失应有的唤
醒就可以了。

在大循环的最后，如果来到这里就表示之前所有尝试都没能获得锁，所以就调用futexsleep让当前
线程挂起，超时时间-1表示会一直睡眠直到被唤醒。

解锁函数的逻辑比较简单，主要通过atomic.Xchg（）函数将l.key替换成mutex_unlocked，然后检查
替换前的旧值，如果等于mutex_sleeping，就通过futexwakeup唤醒一个线程。感兴趣的读者可自行
查看源码，这里不再赘述。

7.4.2 semaphore

runtime中的semaphore是可供协程使用的信号量实现，预期用它来提供一组sleep和wakeup原语，目
标与Linux的futex相同。也就是说，不要把它视为信号量，而是应把它当成实现睡眠和唤醒的一种
方式。每个sleep都与一次wakeup对应，即使因为竞争的关系，wakeup发生在sleep之前。

semaphore的核心逻辑是通过semacquire1（）函数和semrelease1（）函数实现的，semacquire1（）
函数用来执行获取操作，函数的原型如下：

参数addr是用作信号量的uint32型变量的地址，lifo表示是否采用LIFO的排队策略。profile与性能分
析相关，表示要进行哪些种类的采样，目前有semaBlockProfile和semaMutexProfile两种。skipframes
用来指示栈回溯跳过runtime自身的栈帧。

semrelease1（）函数用来执行释放操作，函数的原型如下：

handoff参数表示是否立即切换到被唤醒的协程。被唤醒的协程会设置到当前P的runnext，如果
handoff为true，则当前协程会通过goyield（）让出CPU，被唤醒的协程会立刻得到调度。

runtime内部会通过一个大小为 251 的semtable来管理所有的semaphore，semtable的定义代码如下：

如果只是一个大小固定的table，则肯定无法管理运行阶段数量不定的semaphore。事实上，runtime
会把semaphore放到平衡树中，而semtable存储的是 251 棵平衡树的根，对应数据结构为semaRoot。
semaRoot的定义代码如下：

lock用来保护这棵平衡树，treap字段是真正的平衡树数据结构的根，nwait字段表明了树中结点的数
量，实际上平衡树的每个节点都是个sudog类型的对象，代码如下：

sudog.g用于记录当前排队的协程，sudog.elem用于存储对应信号量的地址。当要使用一个信号量
时，需要提供一个记录信号量数值的变量，根据它的地址addr进行计算并映射到semtable中的一棵

平衡树上，semroot（）函数专门用来把addr映射到对应平衡树的根，代码如下：

它先把addr转换成uintptr，然后对齐到 8 字节，再对表的大小取模，结果用作数组下标。定位到某棵
平衡树之后，再根据sudog.elem存储的地址与信号量变量的地址是否相等，进一步定位到某个节
点，这样就能找到该信号量对应的等待队列了。

如图7-12所示，semtable中序号为 0 的平衡树包括 5 个节点，代表有 5 个不同的信号量通过地址计算并
映射到这棵平衡树，而sudog节点d、e、f属于同一个信号量的等待队列，通过sudog.waitlink和
sudog.waittail连接起来。

semacquire1（）函数会先通过调用cansemacquire（）函数来判断能否在不等待的情况下获取信号
量，该函数的源码如下：

图7-12 semtable示例结构

其实很简单，在信号量的值大于 0 的前提下，循环尝试将信号量的值原子性地减 1 。如果成功了就返

回值true，上一层的semacquire1（）函数也就可以直接返回了。如果在减 1 之前发现信号量的值已
经是 0 了，就返回值false，上一层的semacquire1（）函数就需要执行后续的排队逻辑了。排队逻辑
是在一个for循环中实现的，因为有可能需要多次尝试，代码如下：

首先对root.lock加锁，然后把root.nwait加 1 ，因为当前协程即将到平衡树中去等待了。再次尝试
cansemacquire（）函数，这个尝试是必要的，因为这期间可能有其他协程释放了信号量，而且要注
意操作nwait和addr的顺序，这里是先把nwait加 1 ，后检测addr中的值，semrelease1中会先把addr中
的值加 1 ，后检测nwait，这样能够避免漏掉应有的唤醒。继续回到调用cansemacquire（）函数这
里，如果返回值为true，也就表明获取了信号量，不需要进入平衡树等待了，因此再把nwait减去
1 ，释放锁，然后跳出循环。若cansemacquire（）函数的返回值为false，就要继续排队的流程。通
过调用root.queue（）方法，把与当前协程关联的sudog节点添加到平衡树中，然后调用gopark（）
函数挂起当前协程。

semacquire1（）函数的核心逻辑基本上就是这些，再来看一下semrelease1（）函数，摘选部分关键
代码如下：

它会先把信号量的值加 1 ，然后判断nwait是否为 0 ，如果没有协程在等待就直接返回了。否则就要
对root.lock加锁，再次判断nwait是否为 0 ，若不为 0 就通过root.dequeue（）方法从队列中取出一个协
程，然后把nwait减去 1 并解锁。后面的代码通过goready（）函数唤醒协程，并按需调用goyield（）
函数，以便让出CPU，这里就不把代码全贴出来了。

关于semaphore的探索就讲解到这里，它是为协程而设计的，也是7.4.3节中要介绍的sync.Mutex的
基础。

7.4.3 sync.Mutex

Mutex这个名称的由来，应该是Mutual Exclusion的前缀组合，俗称互斥体或互斥锁。它是一把结合
了自旋锁与信号量的优化过的锁，先来看一下Go语言sync包中Mutex的数据结构，代码如下：

因为足够简单，所以不需要额外的初始化，此结构的零值就是一个有效的互斥锁，处于Unlocked状
态。state存储的是互斥锁的状态，加锁和解锁方法都是通过atomic包提供的函数原子性地操作该字
段。那么，加锁失败时该如何排队等待这个Mutex呢？答案就是7.4.2节介绍的信号量。这里的sema
字段用作信号量，为Mutex提供等待队列。

1 ． Mutex 工作模式

Mutex有两种模式：正常模式和饥饿模式。正常模式下，一个尝试加锁的goroutine会先自旋几次，
尝试通过原子操作获得锁，若几次自旋之后仍不能获得锁，则通过信号量排队等待。所有的等待者
会按照先入先出（FIFO）的顺序排队，但是当一个等待者被唤醒后并不会直接拥有锁，而是需要
和后来者（处于自旋阶段，尚未排队等待的协程）竞争。

这种情况下后来者更有优势，一方面原因是后来者正在CPU上运行，自然比刚被唤醒的goroutine更
有优势，另一方面处于自旋状态的goroutine可以有很多，而被唤醒的goroutine每次只有一个，所以
被唤醒的goroutine有很大概率获得不到锁，这种情况下它会被重新插入队列的头部，而不是尾部。
当一个goroutine本次加锁等待的时间超过了1ms后，它会把当前Mutex切换至饥饿模式。

在饥饿模式下，Mutex的所有权从执行Unlock的goroutine直接传递给等待队列头部的goroutine。后
来者不会自旋，也不会尝试获得锁，它们会直接从队列的尾部排队等待，即使Mutex处于Unlocked
状态。

当一个等待者获得了锁之后，它会在以下两种情况时将Mutex由饥饿模式切换回正常模式：

（ 1 ）它是最后一个等待者，即等待队列空了。

（ 2 ）它的等待时间小于1ms，也就是它刚来不久，后面自然更没有饥饿的goroutine了。

正常模式下Mutex有更好的性能，但是饥饿模式对于防止尾端延迟（队列尾端的goroutine迟迟抢不
到锁）来讲特别重要。

综上所述，在正常模式下自旋和排队是同时存在的，执行Lock的goroutine会先一边自旋一边通过原
子操作尝试获得锁，尝试过几次后如果还没获得锁，就需要去排队等待了。这种在排队之前，先让
大家来抢的模式，能够有更高的吞吐量，因为频繁地挂起、唤醒goroutine会带来较多的开销，但是
又不能无限制地自旋，要把自旋的开销控制在较小的范围内，而饥饿模式下不再自旋尝试，所有
goroutine都要排队，严格地按先来后到执行。

2 ． Mutex 的状态

与Mutex的state字段相关的几个常量定义如下：

mutexLocked表示互斥锁处于Locked状态。mutexWoken表示已经有goroutine被唤醒了，当该标志位
被设置时，Unlock操作不会唤醒排队的goroutine。mutexStarving表示饥饿模式，该标志位被设置时
Mutex工作在饥饿模式，清零时Mutex工作在正常模式。mutexWaiterShift表示除了最低 3 位以外，
state的其他位用来记录有多少个等待者在排队。Mutex.state标志位如图7-13所示。

图7-13 Mutex.state标志位

3 ． Lock （）和 Unlock （）方法

精简了注释和部分与race检测相关的代码，两个方法的代码如下：

这两个方法主要通过atomic函数实现了Fast path，相应的Slow path被单独放在了lockSlow（）方法
和unlockSlow（）方法中。根据源码注释的说法，这样是为了便于编译器对Fast path进行内联优
化。

1 ）Fash path

Lock（）方法的Fast path期望Mutex处于Unlocked状态，没有goroutine在排队，更不会饥饿。理想
状况下，一个CAS操作就可以获得锁了。如果CAS操作没能获得锁，就需要进入Slow path了，也就
是lockSlow（）方法。

Unlock（）方法同理，首先通过原子操作从state中减去mutexLocked，也就是释放锁，然后根据
state的新值来判断是否需要执行Slow path。如果新值为 0 ，也就意味着没有其他goroutine在排队，

所以不需要执行额外操作；如果新值不为 0 ，则可能需要唤醒某个goroutine。

2 ）Slow path

lockSlow（）方法的逻辑比较复杂，需要整体上来理解，笔者通过注释对关键代码进行解释，代码
如下：

然后是与之对应的unlockSlow（）函数的代码如下：

3 ）自旋
再来看一下与自旋相关的函数，首先是判断能否自旋的sync.runtime_canSpin（）函数，它实际上是
个名字链接，真正调用的是runtime.sync_runtime_canSpin（）函数，代码如下：

sync.Mutex是协作式的，在自旋方面比较保守。自旋的次数比较少，并且需要同时满足以下条件：
在一个多核机器上运行并且GOMAXPROCS＞ 1 ，并且至少有一个其他的P正在运行，此外，当前P
的本地runq是空的。

不像runtime.mutex那样，这里不会进行被动（消极）自旋，因为全局runq或者其他P上或许还有可
运行的任务。

sync.runtime_doSpin（）函数也是通过linkname机制链接到runtime.sync_runtime_doSpin（）函数
的，真正的逻辑是通过procyield（）函数实现 30 次自旋。

4 ）信号量相关操作

7.4.2节已经介绍过semaphore，这里只简单看一下调用关系。sync.runtime_Semacquire-Mutex（）函
数是个名字链接，实际上调用的是runtime.sync_runtime_SemacquireMutex（）函数，后者又会调用
runtime.semacquire1（）函数。semacquire1（）函数在7.4.2节已经分析过了，它实现了排队入列逻
辑，通过lifo参数可以实现FIFO和LIFO，实际上就是插入队列尾部还是头部。

sync.runtime_Semrelease（）函数也是个名字链接，实际上调用的是
runtime.sync_runtime_Semrelease（）函数，后者又会调用runtime.semrelease1（）函数。
semrelease1（）函数实现了排队出列逻辑，通过handoff参数可以让被唤醒的goroutine继承当前时间
片并立刻开始运行。

7.4.4 channel

channel被设计用于实现goroutine间的通信，按照golang的设计思想：以通信的方式共享内存。因为
channel在设计上就已经解决了同步问题，所以程序逻辑只要保证数据的所有权随通信传递就可以
了。本节就来分析一下channel实现的原理，先从内存布局开始。

1 ． channel 内存布局

make（）函数会在堆上分配一个runtime.hchan类型的数据结构，示例代码如下：

ch是存在于函数栈帧上的一个指针，指向堆上的hchan数据结构。为什么是堆上的一个结构体？首
先，要实现channel这样的复杂功能，肯定不是几字节可以实现的，所以需要一个struct实现；其
次，这种被设计用于实现协程间通信的组件，其作用域和生命周期不可能仅限于某个函数内部，所
以golang直接将其分配在堆上。

接下来就结合在channel中的作用，解读一下hchan中都有哪些字段。协程间通信肯定涉及并发访
问，所以要有锁来保护整个数据结构，代码如下：

channel分为无缓冲和有缓冲两种，对于有缓冲channel来讲，需要有相应的内存来存储数据，实际
上就是一个数组，需要知道数组的地址、容量、元素的大小，以及数组的长度，也就是已有元素的
个数，这几个字段的代码如下：

因为runtime中内存复制、垃圾回收等机制依赖数据的类型信息，所以hchan中还要有一个指针，指
向元素类型的类型元数据，代码如下：

channel支持交替地读写（比起发送和接收，笔者更喜欢称send为写，称recv为读），有缓冲channel
内的缓冲数组会被作为一个环形缓冲区使用，当下标超过数组容量后会回到第 1 个位置，所以需要
有两个字段记录当前读和写的下标位置，代码如下：

当读和写操作不能立即完成时，需要能够让当前协程在channel上等待，当条件满足时，要能够立
即唤醒等待的协程，所以要有两个等待队列，分别针对读和写，代码如下：

channel是能够被关闭的，所以要有一个字段记录是否已经关闭了，代码如下：

最后整合起来，runtime.hchan结构的代码如下：

至此，我们已经了解了channel的主要数据结构，从各个字段的作用基本就能了解到channel内部大
致是如何运作的。接下来还是结合源码，分析一下send、recv和select都是如何实现的。

2 ． channel 的 send 操作

1 ）阻塞式send操作

首先来看一下channel的常规send操作。假如有一个元素类型为int的channel，变量名为ch，常规的
send操作的代码如下：

其中ch可能有缓冲，也可能无缓冲，甚至可能为nil。按照上面的写法，有两种情况能使send操作不
会阻塞：

（ 1 ）通道ch的recvq里已有goroutine在等待。

（ 2 ）通道ch有缓冲，并且缓冲区没有用尽。

在第一种情况中，只要ch的recvq中有协程在排队，当前协程就直接把数据交给recvq队首的那个协
程就好了，然后两个协程都可以继续执行，无关ch有没有缓冲。在第二种情况中，ch有缓冲，并且
缓冲区没有用尽，也就是底层数组没有存满，此时当前协程直接把数据追加到缓冲数组中，就可以
继续执行。

同样是上面的写法，有 3 种情况会使send操作阻塞：

（ 1 ）通道ch为nil。

（ 2 ）通道ch无缓冲且recvq为空。

（ 3 ）通道ch有缓冲且缓冲区已用尽。

在第一种情况中，参照目前的实现，允许对nil通道执行send操作，但是会使当前协程永久性地阻塞
在这个nil通道上，因死锁抛出异常的示例代码如下：

在第二种情况中，ch为无缓冲通道，recvq中没有协程在等待，所以当前协程需要到通道的sendq中
排队。第三种情况中，ch有缓冲且已用尽，隐含的信息就是recvq为空，不会出现缓冲区不为空且
recvq也不为空的情况，所以当前协程只能到sendq中排队。

2 ）非阻塞式send

接下来再看一看channel的非阻塞式send操作。熟悉并发编程的读者应该知道，有些锁支持tryLock
操作，也就是我想获得这把锁，但是万一已经被别人获得了，我不阻塞等待，可以去做其他事情。
对于channel的非阻塞send就是：我想通过channel发送数据，但是如果当前没有接收者在排队等
待，并且缓冲区没有剩余空间（包含无缓冲的情况），我就需要阻塞等待，但是我不想等待，所以
立刻返回并告诉我“现在不能发送”就可以了。

对于单个通道的非阻塞send操作可以用如下代码实现，注意是一个select、一个case和一个default，
哪个都不能少，代码如下：

如果检测到ch发送数据不会阻塞，就会执行case分支，如果会阻塞，就会执行default分支。

3 ）环形缓冲区

我们通过一个简单例子介绍一下channel的数据缓冲区是如何使用的，为什么称它为环形缓冲区。

假如有一个元素类型为int的channel，缓冲区大小为 5 ，目前sendq和recvq为空，缓冲区还有一个元
素的空闲位置，此时，读下标recvx及写下标sendx的位置如图7-14所示。

图7-14 示例channel读、写下标位置

接下来，有一个goroutine接收了一个元素，被读取的元素就是读下标所指向的第 0 个元素 1 ，此时，
channel缓冲区还有 3 个元素与两个空闲空间，读写下标位置如图7-15所示。

接下来，又有一个goroutine向这个channel发送了一个元素 5 ，此时缓冲区的读写下标位置如图7-16
所示。可以看到新的元素 5 被添加到最后一个空位处，但由于这是缓冲区最后一个位置，所以sendx
回到了缓冲区头部，指向第 0 个位置。此时缓冲区还有 4 个元素与一个空闲位置。

图7-15 读取一个元素后读、写下标位置
图7-16 发送一个元素后读、写下标位置
下面又有两个元素 6 和 7 发送到这个channel，元素 6 会占用此时sendx指向的第 0 个位置，此时，读、
写下标相等，没有空闲位置了，表明缓冲区已满，发送元素 7 的goroutine只能进到sendq中排队等
待，如图7-17所示。

此时排队等待要发送元素 7 的goroutine，只有等到有goroutine从这个channel读取数据后腾出空闲缓
冲区位置，才能完成数据发送。例如接下来读取一个元素，recvx向后移动一个位置，元素 7 被存到
空出的位置，sendq再次为空，缓冲区依然是满的，如图7-18所示。

图7-17 又发送两个元素后缓冲区与sendq的状态

图7-18 读取一个元素后读、写下标和sendq的状态

图7-19 环形缓冲区示意图
可以看到，这个channel缓冲区的读写下标都是从 0 到 4 再到 0 这样循环变化的，这就好像在使用一个
环形缓冲区一样，例如图7-15所示的缓冲区对应图7-19所示的环形缓冲区，灰色区域代表已使用缓
冲区，空白区域代表未使用缓冲区。

3 ． send 操作的源码分析

channel的常规send操作会被编译器转换为对runtime.chansend1（）函数的调用，后者内部只是调用
了runtime.chansend（）函数。非阻塞式的send操作会被编译器转换为对runtime.selectnbsend（）函
数的调用，后者也仅仅调用了runtime.chansend（）函数，所以send操作主要通过chansend（）函数
实现，接下来我们就来分析一下这个函数的源码。chansend（）函数的原型如下：

其中，c是一个hchan指针，指向要用来send数据的channel。ep是一个指针，指向要被送入通道c的
数据，数据类型要和c的元素类型一致。block表示如果send操作不能立即完成，是否想要阻塞等
待。callerpc用以进行race相关检测，暂时不需要关心。返回值为true表示数据send完成，false表示
目前不能发送，但因为不想阻塞（block为false）而返回。

这个函数的逻辑还算比较直观，接下来就分块梳理一下。以下省略掉了部分不太重要的代码，摘选
主要逻辑，第一部分代码如下：

如果c为nil，进一步判断block：如果block为false，则直接返回false，表示未发送数据。如果block为
true，就让当前协程永久地阻塞在这个nil通道上。

第二部分代码如下：

如果block为false且closed为 0 ，也就是在不想阻塞且通道未关闭的前提下，如果通道满了（无缓冲
且recvq为空，或者有缓冲且缓冲已用尽），则直接返回false。本步判断是在不加锁的情况下进行
的，目的是让非阻塞send在无法立即完成时能真正不阻塞（加锁操作可能阻塞）。

第三部分代码如下：

对hchan加锁，如果closed不为 0 ，即通道已经关闭，则先解锁，然后panic。因为不允许用已关闭的
通道进行send。

第四部分代码如下：

如果recvq不为空，隐含了缓冲区为空，就从中取出第 1 个排队的协程，将数据传递给这个协程，并
将该协程置为ready状态（放入run queue，进而得到调度），然后解锁，返回值为true。

第五部分代码如下：

通过比较qcount和dataqsiz判断缓冲区是否还有剩余空间，在这里无缓冲的通道被视为没有剩余空
间。如果有剩余空间，就将数据追加到缓冲区中，相应地移动sendx，增加qcount，然后解锁，返
回值为true。

第六部分代码如下：

运行到这里表明通道已满，如果block为false，即不想阻塞，则解锁，返回值为false。

第七部分代码如下：

当前协程把自己追加到通道的sendq中阻塞排队，gopark（）函数挂起协程后会调用
chanparkcommit（）函数对通道解锁，等到有接收者接收数据后，阻塞的协程会被唤醒。
chansend（）函数在向recvq中的协程发送数据时，调用了send（）函数，send（）函数的主要代码
如下：

其中，数据传递工作是通过sendDirect（）函数完成的，然后调用unlockf（）函数会把hchan解锁，
最后通过goready（）函数唤醒接收者协程。因为发送数据会访问接收者协程的栈，所以
sendDirect（）函数用到了写屏障，函数的代码如下：

至此，channel的send操作就基本告一段落了，接下来我们再来看一看recv操作。

4 ． channel 的 recv 操作

1 ）阻塞式recv

先来看一下channel的常规recv操作。假如有一个元素类型为int的channel，变量名为ch，常规的recv
操作的代码如下：

其中ch可能有缓冲，也可能无缓冲，甚至可能为nil。按照上面的写法，有两种情况能使recv操作不
会阻塞：

（ 1 ）通道ch的sendq里已有goroutine在等待。

（ 2 ）通道ch的sendq是空的，但是通道有缓冲且缓冲区中有数据。

在第一种情况中，只要ch的sendq中有协程在排队，就需要进一步判断通道是否有缓冲：如果无缓
冲，当前协程就直接从sendq队首的那个协程获取数据，然后两者都可以继续执行。如果有缓冲，
隐含信息就是缓冲区已满，否则sendq中不会有协程排队，这时当前协程从缓冲区取出第 1 个数据
（缓冲区有了一个空闲位置），然后从sendq中取出第 1 个协程，把它的数据追加到缓冲区中，并把
它置成ready状态，最终两个协程都能继续执行了。

在第二种情况中，ch的sendq中没有协程在排队，所以不需要关心。如果ch有缓冲，并且缓冲区有
数据，则当前协程直接从缓冲区取出第 1 个数据，然后就可以继续执行了。

同样是上面的写法，有 3 种情况会使recv操作阻塞：

（ 1 ）通道ch为nil。

（ 2 ）通道ch无缓冲且sendq为空。

（ 3 ）通道ch有缓冲且缓冲区无数据。

在第一种情况中，参照目前的实现，允许对nil通道执行recv操作，但是会使当前协程永久性地阻塞
在这个nil通道上，因死锁抛出异常的示例代码如下：

在第二种情况中，ch为无缓冲通道，sendq中没有协程在等待，所以当前协程需要到通道的recvq中
排队。在第三种情况中，ch有缓冲但是没有数据，隐含的信息是sendq为空，否则缓冲区不可能没
有数据，所以当前协程只能到recvq中排队。

2 ）非阻塞式recv

再来看一下channel的非阻塞recv操作。还是类似于tryLock操作，我想获得这把锁，但是万一已经被
别人获得了，我不阻塞等待，可以去做其他事情。对于通道的非阻塞recv就是：我想从通道接收数
据，但是当前没有发送者在排队等待，并且缓冲区内无数据（包含无缓冲），我需要阻塞等待，但
是我不想等待，所以立刻返回并告诉我“现在无数据”就可以了。

对于单个通道的非阻塞recv操作可以用如下代码实现，注意是一个select、一个case和一个default，
哪个都不能少，代码如下：

如果检测到ch recv不会阻塞，就会执行case分支，如果会阻塞，就会执行default分支。

事实上，channel的常规recv操作会被编译器转换为对runtime.chanrecv1（）函数的调用，后者内部
只是调用了runtime.chanrecv（）函数。comma ok写法会被编译器转换为对runtime.chanrecv2（）函
数的调用，内部也是调用chanrecv（）函数，只不过比chanrecv1（）函数多了一个返回值。非阻塞
式的recv操作会被编译器转换为对runtime.selectnbrecv（）函数或selectnbrecv2（）函数的调用（根
据是否comma ok），后两者也仅仅调用了runtime.chanrecv（）函数，所以recv操作主要通过
chanrecv（）函数实现，接下来我们就来分析一下这个函数的源码。

5 ． recv 操作的源码分析

上面简单地分析了channel的常规recv操作和非阻塞recv操作，虽然两者在形式上看起来稍微有些差
异，但是主要逻辑都是通过runtime.chanrecv（）函数实现的，下面简单地进行一下解读。
chanrecv（）函数的原型如下：

其中，c是一个hchan指针，指向要从中recv数据的channel。ep是一个指针，指向用来接收数据的内
存，数据类型要和c的元素类型一致。block表示如果recv操作不能立即完成，是否想要阻塞等待。

selected为true表示操作完成（可能因为通道已关闭），false表示目前不能立刻完成recv，但因为不
想阻塞（block为false）而返回。received为true表示数据确实是从通道中接收的，不是因为通道关
闭而得到的零值，为false的情况需要结合selected来解释，可能是因为通道关闭而得到零值
（selected为true），或者因为不想阻塞而返回（selected为false）。

chanrecv（）函数的大致逻辑与chansend（）函数的大致逻辑很相似，接下来还是省略不太重要的
代码，对函数的主要逻辑分段进行梳理。

第一部分代码如下：

如果c为nil，进一步判断block：如果block为false，就直接返回两个false，表示未recv数据。如果
block为true，就让当前协程永久地阻塞在这个nil通道上。

第二部分代码如下：

如果block为false，也就是在不想阻塞的前提下，并且通道是空的（无缓冲且sendq为空，或者通道
有缓冲且缓冲区为空），就再判断通道是否已关闭。如果未关闭，则直接返回两个false，表示因不
想阻塞而返回。已关闭就先把ep清空，然后返回true和false，表明因通道关闭而得到零值。本步判
断是在不加锁的情况下进行的，目的是让非阻塞recv在无法立即完成时能真正不阻塞（加锁可能阻
塞）。是否为空和是否已关闭这两个判断顺序不能打乱，要在后面判断通道是否关闭。因为关闭后
的通道不能再被打开，这样保证了并发条件下的一致性。如果把判断closed前置，则在检查缓冲区
和sendq时通道可能已关闭，这样会出现错误。

第三部分代码如下：

如果closed不为 0 ，即通道已经关闭，则解锁，然后给ep赋零值，返回值为true和false。

第四部分代码如下：

如果sendq不为空，就从中取出第 1 个排队的协程sg。如果有缓冲，则还需要滚动缓冲区，完成数据
读取，并将协程sg置为ready状态（放入run queue，进而得到调度），然后解锁，这些工作都由
recv（）函数完成。最后返回两个true。

第五部分代码如下：

通过qcount判断缓冲区是否有数据，在这里无缓冲的通道被视为没有数据，因为到达这一步sendq
一定为空。如果缓冲区有数据，将第 1 个数据取出并赋给ep，移动recvx，递减qcount，解锁，返回
两个true。

第六部分代码如下：

运行到这里就说明sendq和缓冲区都为空，如果block为false，也就是不想阻塞，则解锁，返回两个
false。

第七部分代码如下：

最后，运行到这里就要阻塞了，当前协程把自己追加到通道的recvq中阻塞排队，gopark（）函数会
在挂起当前协程后调用chanparkcommit（）函数解锁，等到后续recv操作完成时协程会被唤醒。

第八部分代码如下：

被唤醒有可能是因为通道被关闭，所以最后的返回值received需要根据被唤醒的原因来判断，若是
因为等到真实数据，则为true，若是因为通道关闭，则为false。chanrecv（）函数在从sendq中的协
程接收数据时，调用了recv（）函数，recv（）函数的主要代码如下：

如果是无缓冲通道，则直接通过recvDirect（）函数进行数据复制。若有缓冲，则同时隐含了缓冲
区已满，这样sendq才会不为空。此时需要对缓冲区进行滚动，把缓冲区头部的数据取出来并接
收，然后把sendq头部协程要发送的数据追加到缓冲区尾部。最后，通过goready（）函数唤醒发送
者协程就可以了。

recvDirect（）函数和sendDirect（）函数类似，因为要访问其他协程的栈，所以在应用写屏障后进
行数据复制，代码如下：

关于channel的recv操作就先探索到这里，建议有兴趣的读者好好阅读一下源码。

6 ． channel 之多路 select

本节第 2 部分和第 4 部分在介绍channel的非阻塞式send和非阻塞式recv时提到过select，但是那只是针
对单个通道的操作。不同的写法对应着不同的底层实现，接下来我们就简单地介绍一下多路select
的用法，以及其底层的实现原理。

多路select指的是存在两个及以上的case分支，每个分支可以是一个channel的send或recv操作。例如
ch1和ch2是两个元素类型为int的channel，示例代码如下：

其中default分支是可选的，上述代码会被编译器转换成对runtime.selectgo（）函数的调用，该函数
的原型如下：

cas0指向一个数组，数组里装的是select中所有的case分支，按照send在前recv在后的顺序。

order0指向一个大小等于case分支数量两倍的uint16数组，实际上是作为两个大小相等的数组来用
的。前一个用来对所有case中channel的轮询操作进行乱序，后一个用来对所有case中channel的加锁
操作进行排序。轮询操作需要是乱序的，避免每次select都按照case的顺序响应，对后面的case来讲
是不公平的，而加锁顺序需要按照固定算法排序，按顺序加锁才能避免死锁。

pc0和race检测相关，这里暂时不用关心。nsends和nrecvs分别表示在cas0数组中执行send操作和recv
操作的case分支的个数。

block表示是否想要阻塞等待，对应到代码中就是，有default分支的不阻塞，反之则会阻塞。

下面来看两个返回值，int型的第 1 个返回值表示最终哪个case分支被执行了，对应cas0数组的下
标。如果因为不想阻塞而返回，则这个值是-1。bool类型的第 2 个返回值在对应的case分支执行的是
recv操作时，用来表示实际接收到了一个值，而不是因为通道关闭得到的零值。

selectgo（）函数的逻辑比之chansend（）函数和chanrecv（）函数的逻辑要复杂一些，但是原理上
是相通的。例如第 7 章/code_7_8.go中，一个协程通过多路select等待ch1和ch2，我们暂且把这个协
程记为g1。

g1执行这个多路select时，会先按照有序的加锁顺序对所有channel加锁，然后按照乱序的轮询顺序
检查所有channel的sendq或recvq，以及缓冲区。当检查到ch1或ch2时，如果发现它的等待队列或缓
冲区不为空，就直接复制数据，进入对应分支。

假如所有channel的操作都不能立即完成，就把当前协程g1添加到所有channel的sendq或recvq中，所
以g1被添加到ch1的recvq，也被添加到ch2的sendq中，如图7-20所示，然后就会调用gopark（）函
数把自己挂起，工作线程挂起当前协程后会调用selparkcommit（）函数解锁所有channel。

图7-20 g1在等待队列中等待ch1和ch2

假如接下来ch1有数据可读了，g1被唤醒，完成从ch1中recv数据后，会再次按照加锁顺序对所有
channel加锁，然后从所有sendq或recvq中将自己移除，最后全部解锁后返回。

selectgo（）函数的代码占用篇幅较大，但主要逻辑还算比较清晰，这里不再逐段进行分析，感兴
趣的读者可自行阅读源码。

7.5 本章小结
本章中，我们从单核环境的并发到多核环境的并行，以及编译阶段与执行阶段的内存乱序，逐个讲
解了同步面临的问题。后面又介绍了编译屏障、内存排序指令等解决方案。重点讲解了原子指令及
自旋锁的实现，因为它们是其他各种锁的基础。最后一节中，从源码层面分析了Go语言中几个关
键的同步组件：runtime.mutex、semaphore、sync.Mutex及channel，希望各位读者有所收获。
