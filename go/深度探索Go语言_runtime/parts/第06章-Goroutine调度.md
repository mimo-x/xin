

第 6 章
goroutine
本章的研究对象是Go语言最广为人知、最亮眼的特性，即goroutine，也就是我们俗称的协程。从
本质上来讲，协程更像是一个用户态的线程，主要就是独立的用户栈加上几个关键寄存器的状态。
事实上，这种技术早在多年以前就已经存在了，例如Windows NT的纤程（Fiber），起码已经存在
了二十多年，但是一直不怎么受关注，几乎也没什么人使用。为什么到了Go语言中，协程就成了
这么了不起的技术了呢？一方面是乘了互联网时代高并发场景的东风，另一方面（也是更关键
的），就是和IO多路复用技术的巧妙结合。

因为在语言层面原生支持协程，让开发人员可以很轻松地应对高并发场景，使Go语言非常适合作
为互联网时代的服务器端开发语言。那么协程到底是一种什么技术呢？为什么能够在目前的服务器
端开发中大放异彩呢？让我们带着这些问题，展开本章的探索之旅。

6.1 进程、线程与协程
想要了解协程，还要从最早的进程说起，再到线程，最后是协程。对比之下才能更容易地理解这些
技术是如何演进的。
6.1.1 进程
对于现代操作系统来讲，进程是一个非常基础的概念。进程包含了一组资源，其中有进程的唯一
ID、虚拟地址空间、打开文件描述符表（或句柄表）等，还有至少一个线程，也就是主线程。最
值得一提的就是虚拟地址空间，本书第 1 章在介绍汇编基础时，也简单地介绍了x86处理器的页表映
射机制。现代操作系统利用硬件提供的页表机制，通过为不同进程分配独立的页表，实现进程间地
址空间的隔离。如图6-1所示，不同进程的地址空间中相同的线性地址addr1，经过页表映射以后，
最终会落到不同的物理页面，对应不同的物理地址。有了进程间地址空间的隔离，一些含有Bug或
者恶意的程序就不能非法访问其他进程的内存了，这样才有安全性可言。

如果要创建一个新的进程，则操作系统需要进行哪些操作呢？以Linux为例，Linux通过clone系统调
用来创建新的进程。clone会为新的进程分配对应的内核数据结构和内核栈，以及分配新的进程
ID，然后复制打开文件描述符表、文件系统信息、信号处理器（Signal Handlers）、进程地址空间
和命名空间。因为复制了父进程的打开文件描述符表，所以子进程可以很方便地继承父进程已经打
开的文件、socket等资源，使像nginx、php-fpm这种多进程的工作模式能够比较方便地实现。

图6-1 进程间地址空间的隔离
操作系统在复制进程的地址空间时，基于Copy on Write技术，避免了不必要的内存复制，但是新进
程还是需要有独立的页表，因此创建大量进程时首先会造成内存方面的显著开销，而后操作系统在
进行调度的时候，切换进程需要同步切换页表，页目录寄存器一经修改，TLB缓存也随即失效，造
成地址转换效率降低，进一步影响性能，所以在技术演进迭代的过程中，多进程模式很快就遇到了
瓶颈，无法充分发挥CPU的计算能力，然而多任务的大趋势是不可阻挡的，于是多线程技术应运而
生。

6.1.2 线程
如果理解了进程的组成，再来看多线程就很容易理解了。原本单线程的进程中只有一个主线程，主
线程再通过线程API创建出其他的线程，这就是所谓的多线程模式了。
在多线程模式下，进程的打开文件描述符表、文件系统信息、虚拟地址空间和命名空间是被进程内
的所有线程共享的，但是每个线程拥有自己的内核数据结构、内核栈和用户栈，以及信号处理器。
线程是进程中的执行体，如图6-2所示，为什么要有一个用户栈和一个内核栈呢？因为我们的线程
在执行过程中经常需要在用户态和内核态之间切换，通过系统调用进入内核态使用系统资源。
对于内核来讲，任何的用户代码都被视为不安全的，可能有Bug或者带有恶意的代码，所以操作系
统不允许用户态的代码访问内核数据。线程进入内核态之后执行的是内核提供的代码，也就是安全
的受信任的代码，但是如果跟用户态代码共用一个栈就会留下安全漏洞，栈上的数据可能会被用户
程序非法读取和篡改，所以要给内核态分配单独的栈，用户态的程序无法访问内核栈。

图6-2 线程的用户栈和内核栈
调度系统切换线程时，如果两个线程属于同一个进程，开销要比属于不同进程时小得多，如图6-3
所示。因为不需要切换页表，相应地，TLB缓存也就不会失效。同一个进程中的多个线程，因为共
享同一个虚拟地址空间，所以线程间数据共享变得十分简单高效，只要做好同步就不会有太大问
题，因此，与多进程模式相比，多线程模式大幅优化了性能，系统的吞吐量也随之显著提升。
但是随着并发量的不断增大，应用程序需要创建越来越多的线程，当系统的线程数量达到十万或百
万级别时，系统又将遭遇性能瓶颈。一方面，线程的内核数据结构、内核栈和用户栈会占用大量的
内存，在线程数量庞大时尤其显著。另一方面，操作系统基于时间片策略来调度所有的线程，在如
此庞大的线程数量下，为了尽量降低延迟，线程每次得以运行的时间片会被压缩，从而造成线程切
换频率增高。如果是IO密集型的应用，就会有更多的切换发生，多数时候还没有用完时间片，就
因为IO等待而挂起了。调度系统切换线程的上下文，本身是有一定开销的，在线程数量适中、时
间片足够大时，切换的频率相对较低，这部分开销可以忽略不计。在线程切换频繁时，调度本身的
开销会占用大量CPU资源，造成系统吞吐量严重下降。
图6-3 同进程间线程切换
问题越来越明了了，看起来我们需要一种更轻量的线程，在设计上需要满足两方面的要求：一是节
省内存空间，让主流服务器的内存大小能够轻松装载十万或百万级这种轻量线程。二是调度代价
低，也就是切换起来更轻快。因为高并发的场景就摆在那里，我们就是需要创建大量线程，并且要
求频繁地切换。有了如此明确的需求，协程就被创造出来了。
6.1.3 协程
笔者最初接触到的协程实现是Windows NT提供的纤程，从开发者文档中发现了关于Fiber的那组
API。Fiber设计得非常有意思，它是完全在用户态实现的，所以不需要对系统内核作任何改动，内
核层面并不知道有纤程这种东西的存在。就这一点来讲，goroutine也是一样的，不与系统内核耦
合。6.1.2节简单介绍了线程的组成，对比着来看纤程，可以认为它就是基于线程的用户态部分做了
一些改造。原本的线程有一个用户栈和一个内核栈，一个单线的执行逻辑在用户态和内核态之间跳
跃。对比来看，纤程只有用户栈（没有内核栈），并且调度相关的数据结构也存储在用户空间中。
线程是被操作系统调度的，主要基于时间片策略进行抢占式调度，而纤程是完全在用户空间实现
的，要靠主动让出的方式来切换。

从具体实现来看，纤程就是一个由入口函数地址、参数和独立的用户栈组成的任务，相当于让线程
可以有多个用户栈，如图6-4所示，在每个用户栈上执行不同的任务。线程能够修改自己的栈指针
寄存器，不仅可以上下移动，还可以直接切换到新的栈，所以实现起来并不困难。有一点需要注意
的是，线程的用户栈是由操作系统负责管理的，一般会预留较大的空间，然后按照实际使用情况逐
渐分配、映射，而纤程（协程）的栈，需要由用户程序自己来管理。
与纤程相关的API已经存在了二十多年，但是很少见到相关的应用案例。为什么一直不温不火呢？
可以从两个方面简单地思考一下。一是新技术本身的易用性与可用性，二是应用新技术后能带来的
效益提升。
从易用性来看，系统提供的纤程API实现了创建、销毁、切换等基本功能，而实际的调度策略需要
开发者自己实现，还是有一定的复杂性的。从效益方面来看，也没有太大诱惑力。我们把计算机执
行的任务分成CPU密集型和IO密集型，CPU密集型任务一般更看重吞吐量，所以要尽量减少上下文
切换，每次直接用完时间片就好了，似乎没有纤程的用武之地，而IO密集型任务，可能会更看重
响应延迟，例如互联网应用的网关，但是当时主要的网络IO模型还是阻塞式IO，动不动直接就让
线程挂起了，也没给纤程留下什么发挥的空间，所以在很长一段时间里，像纤程这种协程技术，更
像是一个实验性质的模型，没有得到太广泛的应用，直到协程遇到了IO多路复用。
图6-4 纤程概念示意图
6.2 IO 多路复用
提到IO多路复用技术，现在已经是老生常谈的技术了。从早期的select、poll，到后来的epoll、
kqueue、event port，这门技术已经发展得非常成熟。应该有很多人是从nginx开始了解IO多路复用
技术的，当然也可能是redis、nio等。本章主要研究goroutine，为什么要把IO多路复用拿出来讲解
呢？因为Go语言是集协程思想和IO多路复用技术之大成者，复杂烦琐的事情都由runtime去处理
了，极大地方便了开发者。那么IO多路复用到底是一种什么样的技术呢？它又解决了什么问题
呢？接下来就带着这两个疑问，概括地了解IO多路复用技术。

早年的服务器程序都是以阻塞式IO来处理网络请求的，造成的最大问题就是会让线程挂起，直至
IO完成才会恢复运行。在这种技术背景下，开发者需要为每个请求创建一个线程，线程数会随着
并发等级直线增加，进而造成系统不堪重负。一个解决思路就是把请求和线程解耦，不要让请求绑
定到一个线程或占用一个线程，然后用线程池之类的技术控制线程的数量。阻塞式IO显然不能满
足这种需求，可以考虑使用非阻塞式IO或IO多路复用。下面就来对比一下这三者的不同。

6.2.1 3 种网络 IO 模型
参考《UNIX网络编程》一书，我们把一个常见的TCP socket的recv请求分成两个阶段：一是等待数
据阶段，等待网络数据就绪；二是数据复制阶段，把数据从内核空间复制到用户空间。对于阻塞式
IO来讲，整个IO过程是一直阻塞的，直至这两个阶段都完成。UNIX系统上的socket默认工作在阻
塞模式下，经典的阻塞式网络IO模型如图6-5所示。

图6-5 经典的阻塞式网络IO模型
如果想要启用非阻塞式IO，需要在代码中使用fcntl（）函数将对应socket的描述符设置成
O_NONBLOCK模式。非阻塞式网络IO模型如图6-6所示，与阻塞模式的不同之处主要体现在第一
阶段，即等待数据阶段。在非阻塞模式下，线程等待数据的时候不会阻塞，从编程角度来看就是
recv（）函数会立即返回，并返回错误代码EWOULDBLOCK（某些平台的SDK也可能是
EAGAIN），表明此时数据尚未就绪，可以先去执行别的任务。程序一般会以合适的频率重复调用
recv（）函数，也就是进行轮询操作。在数据就绪之前，recv（）函数会一直返回错误代码
EWOULDBLOCK。等到数据就绪后，再进入复制数据阶段，从内核空间到用户空间。因为非阻塞
模式下的数据复制也是同步进行的，所以可以认为第二阶段也是阻塞的。总之，与阻塞式IO相
比，这里只有第二阶段是阻塞的。

非阻塞式IO看起来比阻塞式要强多了，因为网络的延迟相对比较高，与计算机执行一两个函数花
费的时间根本不在一个数量级，因此在整个IO操作过程中，第二阶段的耗时跟第一阶段相比几乎

是无足轻重的。那么有了非阻塞式IO是不是就万事大吉了呢？实则不然，从图6-6就可以看出来，
虽然第一阶段不会阻塞，但是需要频繁地进行轮询。一次轮询就是一次系统调用，如果轮询的频率
过高就会空耗CPU，造成大量的额外开销，如果轮询频率过低，就会造成数据处理不及时，进而使
任务的整体耗时增加。
IO多路复用技术就是为解决上述问题而诞生的，如图6-7所示，IO多路复用集阻塞式与非阻塞式之
所长。与非阻塞式IO相似，从socket读写数据不会造成线程挂起。在此基础之上把针对单个socket
的轮询改造成了批量的poll操作，可以通过设置超时时间选择是否阻塞等待。只要批量socket中有
一个就绪了，阻塞挂起的线程就会被唤醒，进而去执行后续的数据复制操作。

图6-6 非阻塞式网络IO模型
图6-7 IO多路复用
就拿Linux上的epoll来讲，在实际编程时，对指定的socket进行读或写操作之前，会先通过
epoll_ctl（）函数把socket的描述符添加到epoll中，然后通过epoll_wait（）函数进行监听等待，等
到其中的socket变成可读、可写时，epoll_wait（）函数就会返回。因为epoll是批量监听的，所以要
比阻塞式IO单个等待高效很多。至于是监听socket可读还是可写，要看epoll_ctl（）函数添加描述
符时指定的事件参数，示例代码如下：

这里就是把描述符fd添加到epfd这个epoll实例中，其中的EPOLLIN表明要监听的是可读事件。把
EPOLLIN换成EPOLLOUT就可以监听可写事件了。epoll_event结构的代码如下：

其中epoll_data_t类型的data字段是给开发者用的，用来存放开发者自定义的数据。等到对应的
socket有IO事件触发时，这些数据会被epoll_wait（）函数返回。epoll_wait（）函数的原型如下：

events参数指向一段可以容纳maxevents个epoll_event结构的内存，这段内存是由开发者来分配的，
epoll_wait（）函数会利用这段内存返回一组epoll_event结构。返回的epoll_event结构的events字段
代表具体发生的IO事件，data字段是由开发者自定义的数据，开发者需要通过它来找到与IO事件关
联的socket。更多具体细节可参阅Linux开发者手册Section 2。

这里需要注意的是，如何理解一个socket的可读、可写状态呢？就TCP通信来讲，每个socket都有自
己配套的收、发缓冲区，发送数据的时候调用send（）函数，实际上先把数据写到了socket的发送
缓冲区中，系统会在合适的时机把数据发送给远程的对端，然后清空socket的发送缓冲区。同理，
对端发送过来的数据会被系统自动存放到socket的接收缓冲区，等待应用程序通过recv（）函数来
读取。通俗地讲，当发送缓冲区被写满的时候，自然不能继续写入数据，此时的socket是不可写
的。等到系统把数据发送出去并在发送缓冲区中腾出空间时，socket就变成了可写的了。同理，当
接收缓冲区中没有任何数据时，socket是不可读的，等到系统收到了远程对端发送的数据并把数据
存放到socket的接收缓冲区后，socket就变成可读的了。通过epoll高效地监听批量socket的状态，避
免了非阻塞式IO频繁轮询地空耗CPU，又不会像阻塞式IO那样每个socket挂起一个线程，从而大大
提高了服务器程序的运行效率。

下面就用一个简单的HTTP GET请求，来实际对比阻塞式IO和基于epoll的IO多路复用有什么差异。

6.2.2 示例对比
我们站在客户端的视角，去除掉不太相关的细节，从TCP连接的建立开始梳理。先来梳理阻塞式
IO。
1 ．阻塞式 IO 下的 GET 请求

阻塞式网络IO的主要流程如下：

（ 1 ）客户端通过connect（）函数发起连接，此时线程会被挂起等待，直到三次握手完成、连接成
功建立（或者出现错误）以后，connect（）函数才会返回，线程继续执行。

（ 2 ）客户端通过send（）函数发送HTTP请求报文，因为GET请求报文一般很小，socket的发送缓
冲区足以装载这些数据，所以线程一般不会阻塞。

（ 3 ）通过recv（）函数读取服务器端返回的数据，因为网络通信的延迟与程序指令执行耗时根本
不在一个数量级，所以在这时接收缓冲区内数据尚未就绪，线程一般会阻塞。

（ 4 ）等到数据从服务器端到达客户端后，recv（）函数完成数据的复制（从内核空间到用户空
间），并返回，然后上层的HTTP协议处理数据，判断传输是否完成，如果未完成，则重复执行第
（ 3 ）步，直至传输完成，然后连接可能会被关闭或复用，这个我们就不关心了。

整体流程如图6-8所示，可以发现，阻塞式IO的逻辑非常清晰，只有单一的一条线，是平铺式的、
顺序执行的。代码写起来很简单，后续也便于维护，只是执行效率不是很高，无法充分发挥服务器
硬件的能力。

2 ．应用 epoll 的 GET 请求

接下来再看一下运用epoll时，一个GET请求是如何执行的。整体流程如图6-9所示。

（ 1 ）先通过fcntl（）函数把要用来发起连接的socket设置成O_NONBLOCK模式，然后使用
connect（）函数发起连接。因为socket是非阻塞的，所以connect（）函数会立即返回
EINPROGRESS，表示连接正在建立中。

（ 2 ）因为我们接下来要发送请求报文，要保证socket是可写的，所以就用epoll_ctl（）函数指定
EPOLLOUT事件把socket描述符添加到epoll中，然后调用epoll_wait（）函数等待连接就绪。

（ 3 ）连接建立完成后，socket的发送缓冲区是空的，也就是可写的，所以epoll_wait（）函数会成
功返回，上层的HTTP协议负责完成请求报文的发送，假设报文较小，只需一次send（）函数调
用。

（ 4 ）接下来需要接收服务器端返回的结果，要保证socket是可读的，将epoll中socket对应的描述符
改为监听EPOLLIN事件。

图6-8 阻塞式IO下一个HTTP GET请求的处理流程
图6-9 应用epoll的GET请求

（ 5 ）调用epoll_wait（）函数等待数据就绪，当服务器端的数据包到达客户端后，epoll_wait（）函
数会成功返回，程序通过recv（）函数读取接收缓冲区中的数据。HTTP协议会判断传输是否完
成，未完成则重复执行本步骤。

基于epoll的处理逻辑就不像阻塞式IO那样简明了，人们一般称之为IO事件循环。整个事件循环重
复地执行epoll_wait（）函数，每次epoll_wait（）函数会返回一组已触发的epoll_event事件，其中
有些是可读事件，有些是可写事件（还可能有一些错误事件，这里暂且忽略）。上层协议需要遍历
每个epoll_event事件来处理与之关联的socket，因此还要记录与socket关联的请求的处理状态，例如
有的socket处于连接建立状态，接下来要发送请求报文，还有的socket读取服务器端返回的数据读
到一半，等下次可读时还要继续读取。epoll_event结构中的data字段用来存储这些信息，我们在通
过epoll_ctl（）函数向epoll中添加socket描述符的时候，会把该socket相关的状态数据都存储在一个
结构体中，并把该结构体的地址赋值给data.ptr，然后与socket的描述符一起添加到epoll中。于是，
程序代码的逻辑就像状态机，状态的转移由IO事件与上层协议逻辑共同决定。

综上所述，IO多路复用的出现确实大大提升了应用程序的网络IO效率，对于高并发量的服务器端
程序来讲，改善尤为明显。带来的问题就是显著提升了编程的难度，按照事件循环的方式实现复杂
的应用逻辑非常烦琐。虽然后来催生了一些事件库来方便开发者进行开发，形成了一种基于回调函
数的编程风格，但还是不够直观和方便。能否有一种技术，让我们既能够像阻塞式IO那样平铺直
叙地书写代码逻辑，又能兼得IO多路复用这样的高性能呢？

6.3 巧妙结合
6.2节中不止一次地提到了IO事件循环，循环也就意味着每次执行之后都会回到原点，从程序执行
的底层来看，也就是指令指针和栈指针的还原。因为一个socket（请求）的生命周期往往要跨多轮
循环，所以循环内部不能在栈上存储socket的状态信息。这其实很好理解，因为IO事件的触发是随
机的，因此每次可读、写的一组socket也是随机的，而栈帧的分配与释放是有严格顺序的，所以无
法把socket的状态存储到栈帧上。

如果为每个socket（请求）分配一个独立的栈是不是就可以了呢？此时应该已经很自然地想到协
程，把每个网络请求放到一个单独的协程中去处理，底层的IO事件循环在处理不同的socket时直接
切换到与之关联的协程栈，如图6-10所示。

这样一来，就把IO事件循环隐藏到了runtime内部，开发者可以像阻塞式IO那样平铺直叙地书写代
码逻辑，尽情地把数据存放在栈帧上的局部变量中，代码执行网络IO时直接触发协程切换，切换
到下一个网络数据已经就绪的协程。当底层的IO事件循环完成本轮所有协程的处理后，再次执行
netpoll，如此循环往复，开发者不会有任何感知，程序却得以高效执行。

关于协程的分析就到这里，Go语言中的协程调度并不只是基于IO事件的，只是笔者认为协程与IO
多路复用这两种技术的结合确实非常巧妙，对于目前的服务器端编程语言、编程框架来讲，应该可
以称得上是最关键的技术了。当然，读者也可以有不同的观点。本章接下来的内容，我们将会围绕
goroutine的调度模型进行更加深入的探索。

图6-10 协程与IO多路复用的结合
6.4 GMP 模型
6.4.1 基本概念
说到Go语言的调度系统，GMP调度模型经常被提起。其中的G指的就是goroutine；M是Machine的
缩写，指的是工作线程；P则是指处理器Processor，代表了一组资源，M要想执行G的代码，必须
持有一个P才行。

简单来讲GMP就是Task、Worker和Resource的关系，G和P都是Go语言实现的抽象度更高的组件，
而对于工作线程而言，Machine一词表明了它与具体的操作系统、平台密切相关，对具体平台的适
配、特殊处理等大多在这一层实现。

6.4.2 从 GM 到 GMP
在早期版本的Go实现中（1.1版本之前），是没有P的，只有G和M，GM模型如图6-11所示。

后来为什么要引入一个P呢？主要因为GM调度模型有几个明显的问题：

图6-11 GM调度模型
（ 1 ）用一个全局的mutex保护着一个全局的runq（就绪队列），所有goroutine的创建、结束，以及
调度等操作都要先获得锁，造成对锁的争用异常严重。根据Go官方的测试，在一台CPU使用率约
为70%的 8 核心服务器上，锁的消耗占比约为14%。

（ 2 ）G的每次执行都会被分发到随机的M上，造成在不同M之间频繁切换，破坏了程序的局部性，
主要原因也是因为只有一个全局的runq。例如在一个chan上互相唤醒的两个goroutine就会面临这种
问题。还有一点就是新创建的G会被创建它的M放入全局runq中，但是会被另一个M调度执行，也
会造成不必要的开销。

（ 3 ）每个M都会关联一个内存分配缓存mcache，造成了大量的内存开销，进一步使数据的局部性
变差。实际上只有执行Go代码的M才真地需要mcache，那些阻塞在系统调用中的M根本不需要，
而实际执行Go代码的M可能仅占M总数的1%。

（ 4 ）在存在系统调用的情况下，工作线程经常被阻塞和解除阻塞，从而增加了很多开销。

为了解决上述这些问题，新的调度器被设计出来。总体的优化思路就是将处理器P的概念引入
runtime，并在P之上实现工作窃取调度程序。M仍旧是工作线程，P表示执行Go代码所需的资源。
当一个M在执行Go代码时，它需要有一个关联的P，当M执行系统调用或者空闲时，则不需要P。
GMP调度模型如图6-12所示。

图6-12 GMP调度模型
通过GOMAXPROCS可以精确地控制P的个数，为了支持工作窃取机制，所有的P被放在同一个数
组中，GOMAXPROCS的变动需要Stop/Start The World来调整P数组的大小。原本在sched中的一些
变量被移动到了P中以实现去中心化，例如gfree list、runq，这样可以大幅减少全局锁争用。M中的
一些和Go代码执行相关的变量也被移动到了P中，例如mcache、stackalloc，如此一来减小了不必要
的资源浪费，也优化了局部性。

1 ．本地 runq 和全局 runq

本地runq和全局runq的使用如图6-13和图6-14所示，当一个G从等待状态变成就绪状态后，或者新
创建了一个G的时候，这个G会被添加到当前P的本地runq。当M执行完一个G后，它会先尝试从关
联的P的本地runq中取下一个，如果本地runq为空，则到全局runq中去取，如图6-13所示，如果全局
runq也为空，如图6-14所示，就会去其他的P那里窃取一半的G过来。

图6-13 本地runq为空到全局runq获取G

图6-14 全局runq也为空窃取其他P的G

2 ． M 的自旋

当一个M进入系统调用时，它必须确保有其他的M来执行Go代码。新的调度器设计引入了一定程度
的自旋，就不用再像之前那样过于频繁地挂起和恢复M了，这会多消耗一些CPU周期，但是对整体
性能的影响是正向的。

自旋分两种：第一种是一个有关联P的M，自旋寻找可执行的G；第二种是一个没有P的M，自旋寻
找可用的P。这两种自旋的M的个数之和不超过GOMAXPROCS，当存在第二种自旋的M时，第一
种自旋的M不会被挂起。
当一个新的G被创建出来或者M即将进行系统调用，或者M从空闲状态变成忙碌状态时，它会确保
至少有一个处于自旋状态的M（除非所有的P都忙碌），这样保证了处于可执行状态的G都可以得
到调度，同时还不会频繁地挂起、恢复M。
这些理论主要节选自Go 1.1调度器的设计文档，添加了一些笔者自己的理解。所谓调度，简单来讲
就是工作线程M如何执行G的问题，具体的实现则包含很多细节，接下来就基于源代码来梳理一下
主要逻辑，先从相关的数据结构开始。

6.5 GMP 主要数据结构
6.5.1 runtime.g

基于Go 1.14版以后的源码，首先来看一下G，也就是goroutine对应的数据结构runtime.g，完整的结
构定义字段较多，这里只从中摘选与调度实现较为密切的部分字段，代码如下：

部分字段的用途如表6-1所示。
表6-1 runtime.g部分字段的用途

其中很多字段被笔者精简了，例如之前讲过的_defer、_panic链表，感兴趣的读者可以去看一看完
整的源代码。以上有几个字段需要重点解释一下。

（ 1 ）stack是个结构体类型，它的定义代码如下：

它是用来描述goroutine的栈空间的，对应的内存区间是一个左闭右开区间[lo，hi）。

（ 2 ）用来存储goroutine执行上下文的sched字段需要格外注意，它与goroutine协程切换的底层实现
直接相关，其对应的gobuf结构代码如下：

sp字段存储的是栈指针，pc字段存储的是指令指针，g用来反向关联到对应的G。ctxt指向闭包对
象，也就是说用go关键字创建协程的时候传递的是一个闭包，这里会存储闭包对象的地址。ret用
来存储返回值，实际上是利用AX寄存器实现类似C函数的返回值，目前只发现panic-recover机制用
到了该字段。lr在arm等架构上用来存储返回地址，x86没有用到该字段。bp用来存储栈帧基址。

（ 3 ）atomicstatus描述了当前G的状态，它主要有如表6-2所示几种取值（省略部分过时无用的状
态）。

表6-2 atomicstatus的取值及其含义

标志位_Gscan与上述的一些状态组合，可以得到_Gscanrunnable、_Gscanrunning、_Gscansyscall、

_Gscanwaiting和_Gscanpreempted这些组合状态。除_Gscanrunning外，其他的组合状态都表示GC正
在扫描goroutine的栈，goroutine没有在执行用户代码，栈的所有权归设置了_GScan标志位的
goroutine所有。_Gscanrunning有些特殊，在GC通知G扫描栈的时候，它被用来短暂地阻止状态变
换，其他方面和_Grunning一样。栈扫描完成后，goroutine将会切换回原来的状态，移除_GScan标
志位。

（ 4 ）waiting对应的sudog结构，留到第 7 章讲同步时再进行深入分析。

6.5.2 runtime.m

接下来讲解GMP中的M，也就是工作线程Machine。对应的数据结构是runtime.m，仍然只摘选部分
与调度相关的字段，代码如下：

部分字段的用途如表6-3所示。
表6-3 runtime.m部分字段的用途

6.5.3 runtime.p

再来看一下GMP中的P，也就是Processor对应的数据结构runtime.p，这里只摘选我们感兴趣的部分
字段，代码如下：

各个字段的主要用途如表6-4所示。
表6-4 runtime.p各个字段的主要用途

status字段有 5 种不同的取值，分别表示P所处的不同状态，如表6-5所示。

表6-5 P的不同状态

6.5.4 schedt

还有最后一个数据结构需要关注，也就是用来保存调度器全局数据的sched变量对应的schedt类型。
就像这个结构的类型名字一样，其中的字段大多数和调度相关，所以就不再进行删减了。摘取Go

1.16版源代码中的schedt结构定义，代码如下：

其中部分字段的主要用途如表6-6所示。
表6-6 schedt部分字段的主要用途

与调度相关的数据结构的介绍就到这里，从其中一些字段的用途就能够大致感受到调度器实现的思
路。接下来，尝试按照不同的阶段或不同的功能模块逐步了解整个调度器。
6.6 调度器初始化
6.6.1 调度器初始化过程
Go程序代码经过build之后，生成的是系统原生的可执行文件。可执行文件一般会有个执行入口，
也就是被加载到内存后指令开始执行的地址。如图6-15所示，这个执行入口在不同平台上不尽相
同。在amd64+linux平台上，使用-buildmode＝exe模式构建出来的可执行文件，其对外暴露的执行
入口是_rt0_amd64_linux，对应runtime源码中的汇编函数_rt0_amd64_linux（），该函数只有一条
JMP指令，用于跳转到汇编函数_rt0_amd64（）。汇编函数_rt0_amd64（）只有 3 条指令，用于立刻
调用runtime.rt0_go（）函数。rt0_go（）函数也是个汇编函数，该函数包含了Go程序启动的大致流
程。

接下来我们就从可执行文件的执行入口开始讲解，一直讲解到程序中的main（）函数，看一看Go
程序是如何开始执行的。以下是rt0_go（）函数的主要逻辑：

（ 1 ）初始化g0的栈区间，检测CPU厂商及型号，按需调用_cgo_init（）函数，设置和检测TLS，将
m0和g0相互关联，并将g0设置到TLS中，如图6-16所示。

（ 2 ）调用runtime.args（）函数来暂存命令行参数以待后续解析。部分系统会在这里获取与硬件相
关的一些参数，例如物理页面大小。

（ 3 ）调用runtime.osinit（）函数，所有的系统都会在这里获取CPU核心数，如果上一步没有成功
获取物理页面大小，则部分系统会再次获取。Linux系统会在这里获取Huge物理页面的大小。

图6-15 可执行文件的内存布局
图6-16 初始化g0栈并关联m0

（ 4 ）调用runtime.schedinit（）函数，就像它的名字那样，这个函数会初始化调度系统，函数的逻
辑较为复杂，相关细节稍后再展开介绍。

（ 5 ）调用runtime.newproc（）函数，创建主goroutine，指定的入口函数是runtime.main（）函数，
这是程序启动后第 1 个真正的goroutine，如图6-17所示。

（ 6 ）调用runtime.mstart（）函数，当前线程进入调度循环。一般情况下线程调用mstart（）函数进
入调度循环后不会再返回。进入调度循环的线程会去执行上一步创建的goroutine，如图6-18所示。

主goroutine得到执行后，runtime.main（）函数会设置最大栈大小、启动监控线程sysmon、初始化
runtime包、开启GC，最后初始化main包并调用main.main（）函数。main.main（）函数是用户代码
的主函数，整个初始化过程至此彻底结束。

图6-17 创建主goroutine

图6-18 主goroutine执行

6.6.2 runtime.schedinit （）函数

在上述整个流程中，调用runtime.schedinit（）函数实际上做了很多事情，需要把这个函数的逻辑梳
理一下，该函数也是通过调用多个其他函数完成操作的，调用的函数及其用途如表6-7所示。

表6-7 runtime.schedinit（）函数调用的函数及其用途

直至runtime.schedinit（）函数执行完，P都已经初始化完毕，此时还没有创建任何goroutine，所有P
的runq都是空的。根据procresize（）函数的逻辑，函数返回后当前线程会和第 1 个P关联，也就是
allp[0]。接下来的runtime.newproc（）函数会创建第 1 个goroutine，并把它放到P的本地runq中，如
图6-19所示。

图6-19 第 1 个goroutine创建后的GMP模型

6.7 G 的创建与退出
跟线程类似，goroutine的创建与退出是两个比较关键的操作。在分析用来创建goroutine的
runtime.newproc（）函数之前，需要先了解几个重要的底层汇编函数。

6.7.1 相关汇编函数
1 ． runtime.systemstack （）函数

首先是runtime.systemstack（）函数，该函数被设计用来临时性地切换至当前M的g0栈，完成某些
操作后再切换回原来goroutine的栈。该函数主要用于执行runtime中一些会触发栈增长的函数，因为
goroutine的栈是被runtime管理的，所以runtime中这些逻辑就不能在普通的goroutine上执行，以免陷
入递归。g0的栈是由操作系统分配的，可以认为空间足够大，被runtime用来执行自身逻辑非常安
全。runtime.systemstack（）函数的代码如下：

函数接收一个没有参数和返回值的Function Value作为参数，静态的函数和闭包都能支持。如果当
前已经处于gsignal或g0的栈上，则systemstack（）函数没有任何作用，就像调用者不使用
systemstack（）函数而直接调用fn（）函数一样，所以是可以嵌套使用的。需要注意的是，当从g0
切换回g的时候，并没有将g0的状态保存到g0.sched中，也就是说每次从g0切换至其他的goroutine
后，g0栈上的内容就被抛弃了，下次切换至g0还是从头开始。

2 ． runtime.mcall （）函数

runtime.mcall（）函数和systemstack（）函数很像，也是切换到系统栈去执行某个Function Value，
但是也有些不同，mcall（）函数不能在g0栈上调用，而且也不会再切换回来，函数的代码如下：

函数会先把当前g的状态保存到g.sched，然后切换至g0栈，用当前g的指针作为参数调用fn（）函
数。这个流程非常适合goroutine将自己挂起，fn（）函数中执行调度逻辑对g进行后续处理。需要
注意该函数预期fn（）函数不会返回，也就是说fn（）函数中的调度逻辑需要选择下一个可执行的
g，并完成切换。如何切换到新的g去执行呢？这就是接下来要介绍的runtime.gogo（）函数。

3 ． runtime.gogo （）函数

runtime.gogo（）函数的代码如下：

函数有一个*gobuf类型的参数，buf.g是要恢复运行的goroutine，gogo（）函数利用gobuf中保存的
状态来还原对应的寄存器，再跳转到buf.pc地址处去执行指令。

既然有longjmp，自然也有与之对应的setjmp，也就是runtime.gosave（）函数。

4 ． runtime.gosave （）函数

runtime.gosave（）函数用来把当前goroutine的执行状态保存到gobuf中，代码如下：

函数取的SP和PC的值就像是刚从gosave（）函数返回，后续如果使用gogo（）函数进行longjmp，
程序会从调用者调用gosave（）函数的下一条指令继续执行。关于gobuf.ctxt，因为创建goroutine时
go关键字后面的Function Value可能是个闭包，所以要依靠ctxt来传递闭包对象。一旦使用gogo（）
函数来恢复执行，gobuf.ctxt就会被清零。

了解了上述几个底层函数之后，阅读与调度相关的源码就会比较方便了。下面就来看一下负责创建
新goroutine的runtime.newproc（）函数。

6.7.2 runtime.newproc （）函数

先来看一个Hello World示例，代码如下：

通过6.6节关于初始化的介绍，我们已经了解了从程序执行入口开始，到main.main（）函数执行的
大致过程。main.main（）函数执行时会通过go关键字创建一个协程，我们姑且把它记为hello
goroutine，这里的go关键字实际上会被编译器转换成对runtime.newproc（）函数的调用。函数的代
码如下：

我们先绘制创建hello goroutine的newproc（）函数调用栈，如图6-20所示，其中有几个要点需要分
别进行说明：

图6-20 创建hello goroutine的newproc（）函数调用栈

（ 1 ）argp指针所指向的位置在栈上位于参数fn之后，就像是newproc（）函数的第 3 个参数。从argp
开始siz字节的数据实际上是fn（）函数的参数，被编译器追加在了栈上参数fn的后面，这一点与
defer机制的runtime.deferproc（）函数一致。从newproc（）函数的原型来看，这些被追加的参数是
不可见的，所以newproc（）函数必须是nosplit，以免移动栈时丢失这些参数。

（ 2 ）通过getcallerpc（）函数获取的是创建者的指令指针，主要被新的goroutine用于记录自己是在
哪里被创建的。

（ 3 ）实际上真正的创建工作是在runtime.newproc1（）函数中完成的，该函数有些复杂，可能会造
成栈增长，同时又有nosplit的限制，所以要通过systemstack（）函数切换至系统栈执行。

（ 4 ）新创建的newg通过runqput（）函数被放置在当前P的本地runq中。mainStarted表示
runtime.main（）函数，即主goroutine已经开始执行，此后才会通过wakep（）函数启动新的工作线
程，以保证main（）函数总会被主线程调度执行。

对于新goroutine的分配及初始化工作，都是在runtime.newproc1（）函数中完成的，该函数的代码
篇幅较大，此处就不将整段代码贴出来了，只在必要的地方节选一些。函数的主要逻辑包含以下几
部分：

（ 1 ）分配新的g，先尝试gfget（）函数从空闲队列中获取，如果没有，再用malg（）函数分配新
的g。

（ 2 ）计算栈上所需空间的大小，用参数的大小加额外预留的空间，还要经过对齐。
（ 3 ）根据上一步的计算确定SP的位置，把参数复制到新g的栈上，需要用到写屏障。

（ 4 ）初始化执行上下文，这里用到gostartcallfn（）函数，稍后会进一步展开介绍。

（ 5 ）将G的状态设置为_Grunnable，并根据当前P的goidcache为g分配ID。

关于新goroutine执行上下文的初始化比较关键，因为初始化过的g会先被放入P的本地runq中，等到
接下来的调度循环中才会被执行。切换到新的goroutine执行会用到runtime.gogo（）函数，也就是
基于g.sched的gobuf来恢复执行现场，所以初始化的时候要在g.sched中模拟出一个执行现场，关键
代码如下：

创建hello goroutine时newproc1（）函数模拟的执行现场如图6-21所示。其中的sp就是从栈底留出参
数及额外空间后的位置，pc的位置比较有意思，是runtime.goexit（）函数的起始地址加上 1 字节
（sys.PCQuantum在amd64上是 1 字节）。这样初始化pc是为了让调用栈看起来像是起始于
goexit（）函数，然后goexit（）函数调用了fn（）函数，也就是hello（）函数。如此一来，当
fn（）函数执行完毕后，会返回goexit（）函数中，goexit（）函数中实现了goroutine结束后退出的
标准逻辑。pc的值之所以需要是goexit（）函数的地址加 1 ，是因为这样才像是goexit（）函数调用
了fn（）函数，如果指向goexit（）函数的起始地址就不合适了，那样goexit（）函数看起来还没有
执行。

图6-21 创建hello goroutine时newproc1（）函数模拟的执行现场

读者可能会担心goexit（）函数地址加 1 会造成指令错乱，实际不会有问题，因为goexit（）函数的
代码已经考虑到这一层了，代码如下：

首尾各有一条NOP指令占位，所以入口地址加 1 后不会有什么影响，正好对齐到了接下来的CALL
指令，而且还可以发现goexit（）函数真正的逻辑是在goexit1（）函数中实现的，这个暂不展开介
绍。接下来继续看goroutine执行上下文的初始化，gostartcallfn（）函数内部调用了gostartcall（）函
数实现了主要功能，x86对应的gostartcall（）函数的源代码如下：

图6-22 hello goroutine创建后的GMP

寄存器大小不等于指针大小的情况可直接忽略，函数的主要逻辑是：先把SP向下移动一个指针大
小，然后把PC的值写入SP指向的内存，这相当于在栈上压入了一个新的栈帧，原PC成为返回地
址。最后更新gobuf的sp和pc字段，新的pc是fn，最终构造的执行现场就像是goexit（）函数刚刚调
用了fn（）函数，刚刚完成跳转还没来得及执行fn（）函数的指令。

至此，runtime.newproc（）函数创建新goroutine的流程大致梳理完了，新goroutine已经被放置到了
P的本地runq中，会在后续的调度循环中得到执行，hello goroutine创建完成后GMP模型如图6-22所
示。

main goroutine在main.main（）函数返回后就会调用exit（）函数结束进程，所以示例代码中的hello
goroutine还没来得及得到调度执行，整个进程就结束了。可以通过等待timer或channel的方式拖延
main.main（）函数的返回时间，这样就可以等到hello goroutine退出后再结束进程了。

至于goroutine的退出，相对而言就比较简单了，runtime.goexit1（）函数实际上也不是主要逻辑实
现的地方，该函数只不过通过mcall（）函数调用了goexit0（）函数。为什么要通过mcall（）函数
调用呢？因为当前goroutine即将退出了，不能继续执行，必须切换至系统栈来完成收尾处理。
goexit0（）函数中才是真正进行收尾的地方，该函数的逻辑比较简单，主要包括以下几个步骤：

（ 1 ）将g的状态置为_Gdead。

（ 2 ）g的一些字段需要做清零处理。

（ 3 ）通过dropg（）函数将g与当前M解绑。

（ 4 ）调用gfput（）函数将g放入空闲队列，以便于复用。

（ 5 ）调用schedule（）函数，调度执行其他已经就绪的goroutine。

其中最后一步调用的runtime.schedule（）函数就是我们通常所讲的调度循环，确切地说应该是调度
循环中的一次循环，工作线程通过不断地调用schedule（）函数来调度执行下一个goroutine。6.8节
将从schedule（）函数入手，梳理一下调度的主要逻辑。

6.8 调度循环
6.8.1 runtime.schedule （）函数

工作线程通过调用runtime.schedule（）函数进行一次调度，该函数就是调度逻辑的主要实现。源代
码稍微有点多，下面分成几部分进行梳理。

第一部分代码如下：

函数开始处先通过getg（）函数获得了当前正在运行的g，执行schedule（）函数时一般都是系统栈
g0。接下来的第 1 个if语句校验当前线程没持有锁，不允许在持有锁的情况下进行调度，以免造成
runtime内部错误，这里的锁是runtime底层的锁，与sync包中的Mutex等不是一个级别。第 2 个if判断
当前M有没有和G绑定，如果有，这个M就不能用来执行其他的G了，只能挂起等待绑定的G得到调
度。第 3 个if判断线程是不是正在进行cgo函数调用，这种情况下g0栈正在被cgo使用，所以也不允许
调度。

第二部分代码如下：

从top标签开始就是真正的调度逻辑了，设置这个标签的目的，是为了后面某些情况下需要goto这
里重来一遍。通过把preempt设置为false，来禁止对P的抢占。检测sched.gcwaiting，挂起自己，以
便及时响应STW，调度逻辑中多个地方都有对gcwaiting的检测。runSafePointFn（）函数被GC用来
在安全点执行清空工作队列之类的操作。最后对spinning的判断属于一致性校验，在P本地runq有任
务的情况下，M不应该处于spinning状态。

第三部分代码如下：

通过checkTimers（）函数处理当前P上的定时器，关于定时器会在6.10节中详细讲解。接下来的两
个if语句块尝试获得待运行的Trace Reader和GC Worker，一般的goroutine切换至就绪状态时会通过
wakep（）函数按需启动新的线程，但是这两者不会，所以通过tryWakeP记录是否需要wakep（）
函数。

第四部分代码如下：

在schedtick能够被 61 整除的时候，优先尝试从全局runq中获取任务，其他情况则只从本地runq中获
取。大致相当于每调度 60 次本地runq，就会调度一次全局runq。这样做是为了在保证效率的基础上
兼顾公平性，否则本地队列上的两个持续唤醒的goroutine会造成全局队列一直得不到调度。如果前
面所有的步骤都没有找到一个待运行的goroutine，就会调用findrunnable（）函数来找任务执行，该
函数会一直阻塞，直到找到可运行的goroutine，而且findrunnable（）函数是个十足的质量级函数，
稍后再进行介绍。代码执行到这里，gp肯定已经不是nil了，如果M处于spinning状态，就要调用
resetspinning（）函数来脱离spinning状态，resetspinning（）函数会调用wakep（）函数按需启动新
的线程。

第五部分代码如下：

至此，虽然已经找到了待运行的g，还要确定目前是否处于禁止调度用户协程的状态。在禁止调度
用户协程的状态下，gp如果是系统协程就可以正常执行，用户协程需要先通过disable队列暂存起
来，调度逻辑跳转到top重新寻找可执行的g。等到允许调度用户协程时，disable队列中的g会被重
新加入runq中。

最后一部分代码如下：

第 1 个if根据tryWakeP来尝试唤醒新的线程，以保证有足够的线程来调度Trace Reader和GC
Worker。第 2 个if判断gp是否有绑定的线程，如果有就必须唤醒绑定的线程来执行gp，而且当前线
程也要回到top再来一遍。若gp没有绑定的M，就通过execute（）函数来执行gp。executre（）函数
会关联gp和当前的M，将gp的状态设置为_Grunning，并通过gogo（）函数恢复执行上下文，这里
不再展开介绍，感兴趣的读者可自行阅读源码。

至此，schedule（）函数就梳理完了，主要逻辑如图6-23所示，整体还算简单明了。我们并没有看
到传说中的任务窃取等逻辑，这些逻辑在哪里呢？那就是接下来要梳理的findrunnable（）函数
了。

图6-23 schedule调度循环主要逻辑

6.8.2 runtime.findrunnable （）函数

findrunnable（）函数的逻辑可以分成前后两部分，前半部分完成了timer触发、netpoll和任务窃取，
后半部分针对的是没有找到任务的情况，会处理GC后台标记任务、按需执行netpoll，实在没有任
务就会挂起等待。findrunnable（）函数的主要逻辑如图6-24所示。

图6-24 findrunnable（）函数的主要逻辑

由于findrunnable（）函数的整体代码量比较大，在这里我们就不全部贴出来了，只把前一半的代
码分几部分进行分析。

第一部分代码如下：

函数的开头处也是要先检测gcwaiting及runSafePointFn，后续逻辑有可能会阻塞，为了避免GC等待
太长时间，检测逻辑被放在了top标签的内部，每次跳转回来都会进行检测。

第二部分代码如下：

调用checkTimers（）函数会运行当前P上所有已经达到触发时间的计时器，这可能会使一些
goroutine从_Gwaiting变成_Grunnable状态。接下来按需唤醒finalizer goroutine，然后检查本地runq
和全局runq中是否有可运行的任务，找到任务就可以直接返回了。

第三部分代码如下：

按需执行一次非阻塞的netpoll，如果返回的列表非空，就把第 1 个g从列表中pop出来，将剩余的插
入全局runq，把这个g的状态置为_Grunnable，然后返回。

第四部分代码如下：

这一大段代码实现了核心的任务窃取逻辑，第 1 个if判断的含义是，如果当前处于spinning状态的M
的数量大于忙碌的P的数量的一半，就让当前M阻塞。目的是避免在GOMAXPROCS较大而程序实
际的并发性很低的情况下，造成不必要的CPU消耗。

任务窃取逻辑会循环尝试 4 次，最后一次才会窃取runnext和timer，也就是说前 3 次只会从其他P的本
地runq中窃取。stealOrder用来实现一个公平的随机窃取顺序，timerpMask和idlepMask用来快速判
断指定位置的P是否有timer或者是否空闲。如果ran为true，表示checkTimers（）执行了其他P的
timer，可能会使某些goroutine变成_Grunnable状态，所以先检查当前P的本地runq，如果没有找到
就跳转回top重来一次。

调度相关的逻辑中会频繁地对runq进行操作，runtime为此专门提供了一组函数，常见的函数例如
runqget（）函数、runqput（）函数等，还有上面的runqsteal（）函数也是其中的一个。这些函数的
逻辑都比较简明，这里只把runqget（）函数的源码分析一下，目的是看一看P的本地队列如何支持
继承时间片，代码如下：

原来是通过runnext字段实现的，只有取自runnext的g对应的inheritTime才是true，其他本地runq中的
g的返回值都为false，也就是不会继承时间片。相应地，如果某个g需要继承时间片，runqput（）函
数就会把它设置到runnext，感兴趣的读者可以自行查看源代码。

本节就讲解到这里，主要对schedule（）函数和findrunnable（）函数进行了简要梳理，基本了解了
每一轮调度循环都会做些什么。应该说schedule（）函数就是调度循环的实现，但是当goroutine开
始执行用户代码后，执行流是如何再回到runtime中去调用schedule（）函数的呢？这就是6.9节中要
探索的抢占式调度。

6.9 抢占式调度
就像操作系统要负责线程的调度一样，Go的runtime要负责goroutine的调度。现代操作系统调度线
程都是抢占式的，我们不能依赖用户代码主动让出CPU，或者因为IO、锁等待而让出，这样会造成
调度的不公平。基于经典的时间片算法，当线程的时间片用完之后，会被时钟中断给打断，调度器
会将当前线程的执行上下文进行保存，然后恢复下一个线程的上下文，分配新的时间片，令其开始
执行。这种抢占对于线程本身是无感知的，由系统底层支持，不需要开发人员特殊处理。

基于时间片的抢占式调度有个明显的优点，能够避免CPU资源持续被少数线程占用，从而使其他线
程长时间处于饥饿状态。goroutine的调度器也用到了时间片算法，但是和操作系统的线程调度还是
有些区别的，因为整个Go程序都运行在用户态，所以不能像操作系统那样利用时钟中断来打断运
行中的goroutine。也得益于完全在用户态实现，goroutine的调度切换更加轻量。

本节就来实际研究一下，runtime到底是如何抢占运行中的goroutine的。为了避免过于枯燥乏味，先
不直接解读源码，而是先做个实验，准备的示例代码如下：

6.9.1 Go 1.13 的抢占式调度

笔者使用的是Go 1.13.15版，build完成后运行得到的是可执行文件。程序会如你所料地运行起来，
飞快地打印出一行行递增的数字。不要着急，让程序多运行一会儿，用不了太长时间你就会发现程
序突然停了，不再继续打印了。在笔者测试的 64 位Linux系统上，最大数字没有超过 500000 ，程序
似乎就停住了。是真的停住了吗？如果用top命令查看，就会发现CPU占用达到100%。也就是说程
序还在运行中，并且占满了一个CPU核心。

为了弄清楚程序到底在做什么，我们使用调试器delve查看一下当前所有的goroutine的状态，执行的
命令如下：

可以看到一共有 8 个goroutine，除了 1 号和 18 号是在执行用户代码外，其他都与GC相关且都处于空
闲或等待状态。 1 号goroutine正在执行main（）函数，main.go的第 12 行就是main（）函数最后空的
for循环，说明它一直在这里循环，占满一个CPU核心的应该就是它。 18 号goroutine执行的位置在
func1（）函数中，对照源码行号来看就是协程中的fmt.Println（）函数。我们通过调试器切换到 18
号goroutine，然后查看它的调用栈，执行的命令如下：

按照这个调用栈，结合我们看到的现象进行分析：协程中要调用fmt.Println（）函数，该函数的参
数类型是interface{}，所以要先调用runtime.convT64（）函数来把一个int64（amd64平台上的int本
质上是int64）转换为interface{}类型，而convT64（）函数内部需要分配内存，经过多次循环之后
达到了GC阈值，要先进行GC才能继续执行，所以mallocgc（）函数调用gcStart（）函数开始执行
GC。后续能够看出gcStart（）函数内部切换至了系统栈，然后发生了等待阻塞。

我们通过源码看一下mgc.go的 1287 行到底在干什么，代码如下：

原来是通过systemstack（）函数切换至系统栈，然后调用stopTheWorldWithSema（）函数，看来是

要STW，但为什么会阻塞呢？这就要讲讲STW的实现原理了。6.5.4节在解释schedt的gcwaiting字段
时有过简单介绍，这里摘选了该函数的核心代码来看一下，代码如下：

先根据gomaxprocs的值设置stopwait，实际上就是P的个数，然后把gcwaiting置为 1 ，并通过
preemptall（）函数去抢占所有运行中的P。preemptall（）函数会遍历allp这个切片，调用
preemptone（）函数逐个抢占处于_Prunning状态的P。接下来把当前M持有的P置为_Pgcstop状态，
并把stopwait减去 1 ，表示当前P已经被抢占了，然后遍历allp，把所有处于_Psyscall状态的P置为
_Pgcstop状态，并把stopwait减去对应的数量。再循环通过pidleget（）函数取得所有空闲的P，都置
为_Pgcstop状态，从stopwait减去相应的数量。最后通过判断stopwait是否大于 0 ，也就是是否还有没
被抢占的P，来确定是否需要等待。如果需要等待，就以100μm为超时时间，在sched.stopnote上等
待，超时后再次通过preemptall（）函数抢占所有P。因为preemptall（）函数不能保证一次就成
功，所以需要循环。最后一个响应gcwaiting的工作线程在自我挂起之前，会通过stopnote唤醒当前

线程，STW也就完成了。
实际用来执行抢占的preemptone（）函数的代码如下：

第 1 个if判断是为了避开当前M，不能抢占自己。第 2 个if用于避开处于系统栈的M，不能打断调度器
自身，而所谓的抢占，就是把g的preempt字段设置成true，并把stackguard0这个栈增长检测的下界
设置成stackPreempt。这样就能实现抢占了吗？

还记不记得之前反编译很多函数的时候，都会看到编译器安插在函数头部的栈增长代码？例如对于
一个递归式的斐波那契函数，代码如下：

经过反编译后，可以看到最终生成的汇编指令如下：
还是转换成等价的Go风格的伪代码更容易理解，也更直观，伪代码如下：

实际上，编译器安插在函数开头的检测代码会有几种不同的形式，具体用哪种形式是根据函数栈帧
的大小来定的。不管怎样检测，最终目的都是一样的，就是避免当前函数的栈帧超过已分配栈空间
的下界，也就是通过提前分配空间来避免栈溢出。
执行抢占的时候，preemptone（）函数设置的那个stackPreempt是个常量，将其赋值给stackguard0之
后，就会得到一个很大的无符号整数，在 64 位系统上是0xfffffffffffffade，在 32 位系统上是
0xfffffade。实际的栈不可能位于这个地方，也就是说SP寄存器始终会小于这个值，因此，只要代
码执行到这里，肯定就会去执行runtime.morestack_noctxt（）函数，而morestack_noctxt（）函数只
是直接跳转到runtime.morestack（）函数，而后者又会调用runtime.newstack（）函数。
newstack（）函数内部检测到如果stackguard0＝stackPreempt这个常量，就不会真正进行栈增长操
作，而是去调用gopreempt_m，后者又会调用goschedImpl（）函数。最终goschedImpl（）函数会调
用schedule（）函数，还记得schedule（）函数开头检测gcwaiting的if语句吗？工作线程就是在那些
地方响应STW的。执行流能够一路走到schedule（）函数，这就是通过栈增长检测代码实现
goroutine抢占的原理。

现在就比较容易理解我们实验程序停住的原因了，执行fmt.Println（）函数的goroutine需要执行
GC，进而发起了STW，而main（）函数中的空for循环因为没有调用任何函数，所以没有机会执行
栈增长检测代码，也就不能被抢占了。

如图6-25所示，Go 1.13版本及之前的抢占依赖于goroutine检测到stackPreempt标识而自动让出，并
不算是真正意义上的抢占。一个空的for循环就让程序挂起了，这可真是个隐患。虽然我们不会在
生产环境写出这种代码，但是对于调度器来讲，毕竟是个缺陷，所以在Go 1.14版本中，这个问题
被解决了。

6.9.2 Go 1.14 的抢占式调度

Go 1.14实现了真正的抢占式调度，从现象来看，还是采用第 6 章/code_6_2.go那个实验代码，用Go
1.14版生成可执行文件，再运行就不会阻塞了。从Go 1.14版开始，空的for循环这类代码也能被抢
占了，就像操作系统通过中断打断运行中的线程一样。

这种真正的抢占是如何实现的呢？在UNIX系操作系统上是基于信号实现的，所以也称为异步抢
占。接下来就以Linux系统为例，实际研究一下。这次需要先从源码开始，对比一下Go 1.14版与Go
1.13版有哪些不同，了解了具体的细节之后再通过调试等手段进行相关实践。

下面就是Go 1.14版runtime.preemptone（）函数的源码，可以看到比之前的Go 1.13版多出来了最后
的那个if语句块，代码如下：

图6-25 Go 1.13版本中的抢占式调度流程

图6-26 Linux系统中异步抢占的前一半工作

其中的preemptMSupported是个常量，因为受硬件特性的限制，在某些平台上是无法支持这种抢占
的。debug.asyncpreemptoff则是让用户可以通过GODEBUG环境变量来禁用异步抢占，默认情况下
是被启用的。在P的数据结构中也新增了一个preempt字段，这里会把它设置为true。实际的抢占操
作是由preemptM（）函数完成的。

preemptM（）函数的主要逻辑就是通过runtime.signalM（）函数向指定M发送sigPreempt信号。至
于signalM（）函数，就是调用操作系统的信号相关系统调用，将指定信号发送给目标线程。至
此，异步抢占逻辑的主要工作就算完成了前一半，如图6-26所示，信号已经发出去了。

异步抢占工作的后一半就要由接收到信号的工作线程来完成了。还是先定位到相应的源码，
runtime.sighandler（）函数就是负责处理接收的信号的，其中有这样一个if语句，代码如下：

如果收到的信号是sigPreempt，就调用doSigPreempt（）函数。doSigPreempt（）函数的代码如下：

重点就在于第 1 个if语句块，它先通过wantAsyncPreempt（）函数确认runtime确实想要对指定的G实
施异步抢占，再通过isAsyncSafePoint（）函数确认G当前执行上下文是能够安全地进行异步抢占
的。实际看一下wantAsyncPreempt（）函数的源码，代码如下：

它会同时检查G和P的preempt字段，并且G当前需要处于_Grunning状态。在每轮调度循环中，P和G
的preempt字段都会被置为false，所以这个检测能够避免刚刚切换至一个新的G后马上又被抢占。
isAsyncSafePoint（）函数的代码比较复杂且涉及较多其他细节，这里就不展示源码了。它从以下
几个方面来保证在当前位置进行异步抢占是安全的：

（ 1 ）可以挂起G并安全地扫描它的栈和寄存器，没有潜在的隐藏指针，而且当前并没有打断一个
写屏障。

（ 2 ）G还有足够的栈空间来注入一个对asyncPreempt（）函数的调用。

（ 3 ）可以安全地和runtime进行交互，例如未持有runtime相关的锁，因此在尝试获得锁时不会造成

死锁。
以上两个函数都确认无误后，才通过pushCall向G的执行上下文中注入一个函数调用，要调用的目
标函数是runtime.asyncPreempt（）函数。这是一个汇编函数，它会先把各个寄存器的值保存在栈
上，也就是先将现场保存到栈上，然后调用runtime.asyncPreempt2（）函数。asyncPreempt2（）函
数的代码如下：

其中preemptStop主要在GC标记时被用来挂起运行中的goroutine，preemptPark（）函数会把当前g切
换至_Gpreempted状态，然后调用schedule（）函数，而通过preemptone（）函数发起的异步抢占会
调用gopreempt_m（）函数，它最终也会调用schedule（）函数。至此，整个抢占过程就完整地实
现了。

关于如何在执行上下文中注入一个函数调用，我们在这里结合AMD64架构做一下更细致的说明。
runtime源码中与AMD64架构对应的pushCall（）函数的代码如下：

先把SP向下移动一个指针大小的位置，把PC的值存入栈上SP指向的位置，然后将PC的值更新为
targetPC。这样就模拟了一条CALL指令的效果，如图6-27所示，栈上存入的PC的旧值就相当于返
回地址。此时整个执行上下文的状态就像是goroutine在被信号打断的位置额外执行了一条CALL
targetPC指令，由于执行流刚刚跳转到targetPC地址处，所以还没来得及执行目标地址处的指令。

当sighandler（）函数处理完信号并返回之后，被打断的goroutine得以继续执行，会立即调用被注入
的asyncPreempt（）函数。经过一连串的函数调用，最终执行到schedule（）函数。异步抢占的后
一半工作流程如图6-28所示。

图6-27 AMD64架构下注入一个函数调用
图6-28 Linux系统中异步抢占的后一半工作

了解了整个流程之后，我们再来做一个很简单的实验。还是采用第 6 章/code_6_2.go文件中的代码，
用Go 1.14版编译之后再运行，可以发现程序会一直输出，不再阻塞。这时，用dlv调试器附加到目
标进程，并且在runtime.asyncPreempt2（）函数中设置断点，然后让程序继续运行。等到命中断点
后，查看调用栈的回溯，命令如下：

从栈回溯来看是main（）函数调用了asyncPreempt（）函数，而main.go的 12 行正是那个空的for循
环，它没有调用任何函数，这个调用就是被pushCall（）函数注入的。

还有一种方式，可以通过GODEBUG环境变量来禁用异步抢占，此时会发现Go 1.14版编译的程序
运行一段时间后也会阻塞，命令如下：

另外还有一点，如果把协程中用来打印的fmt.Println（）函数换成println（）函数，则会发现运行
很久都不会阻塞，即使是Go 1.13版编译的程序也是如此。这是因为println（）函数不需要额外分配
内存，感兴趣的读者可以自行尝试。本节关于抢占式调度的探索就讲解到这里。

6.10 timer
6.10.1 一个示例
在6.7.2节介绍协程创建时我们使用了一个hello goroutine的例子，其中main goroutine创建的hello
goroutine还没执行，main.main（）函数就返回了，然后exit（）函数就结束了进程。下面我们让
main goroutine在timer中等待一下，让hello goroutine有时间得以运行，代码如下：

当main goroutine执行到time.Sleep（）函数时，会创建一个timer对象，timer对象会记录timer的触发
时间和时间到达时需要执行的回调函数，以及是哪个协程在等待timer等信息。

设置好timer对象后，就会调用gopark（）函数，使当前goroutine挂起，让出CPU。main goroutine的
状态会从_Grunning改为_Gwaiting，不会进入当前P的本地runq，而是进到刚刚创建的那个timer中
等待，随后hello goroutine有机会得到调度执行，如图6-29所示。

图6-29 main goroutine等待timer时hello goroutine得到调度

等到timer触发时间到达后，回调函数timer.f（）得以执行，对于time.Sleep（）函数而言，timer.f被
设置为goroutineReady（）函数，函数的代码如下：

goroutineReady（）函数直接调用goready（）函数，它会切换到g0栈，并执行runtime.ready（）函
数。待ready的协程自然是main goroutine，此时它的状态是_Gwaiting，接下来会被修改为
_Grunnable，表示它又可以被调度执行了，然后，它会被放到当前P的本地runq中，所以，timer等
待的时间到达后，main goroutine又可以得到调度执行了。接下来，在main goroutine恢复执行后，
main.main（）函数执行后返回，进程退出。

通过这个修改后的例子，我们初步了解了协程等待timer时让出与恢复的大致过程，接下来我们展
开一些细节。

6.10.2 数据结构
在runtime中，每个计时器都用一个timer对象表示。在Go 1.14版本及后续的版本中，timer的结构定
义代码如下：

其中各个字段的用途如表6-8所示。
表6-8 timer数据结构各个字段的用途

针对status字段，runtime源码中定义了 10 种状态，如表6-9所示。

表6-9 status不同状态的含义

在runtime.p中也有一组专门用来支持timer的字段，节选的相关代码如下：

其中各个字段的用途如表6-10所示，timerModifiedEarliest是在Go 1.16版本中新增的，其余字段都是
在Go 1.14版本中重构timer时引入的。

表6-10 runtime.p中支持timer的字段及其用途

6.10.3 操作函数
runtime中有一组与timer相关的函数，其中有最底层的siftupTimer（）函数和siftdownTimer（）函
数，它们用来在最小堆中根据when字段的值上下移动timer，以维持堆的有序性。doaddtimer（）函
数、dodeltimer（）函数及dodeltimer0（）函数用来将指定的timer添加到堆中，或者将其从堆中移
除，这也是偏底层一些的函数，不会被runtime中除timer以外的其他模块直接调用。像

addtimer（）、deltimer（）、modtimer（）和resettimer（）这几个函数就属于timer模块提供的接口
了，runtime中的其他模块可以直接调用这些函数，例如6.11节要讲解的netpoller就会用到这组函
数。至于startTimer（）、stopTimer（）和resetTimer（）这些函数，只是对这组接口函数进行了简
单包装，并通过linkname机制链接到time包，提供给标准库使用。最后，还有被调度器调用的
adjusttimers（）函数、runtimer（）函数和clearDeletedTimers（）函数，它们会对timer堆进行维
护，以及运行那些到达触发时间的timer。

在上述函数中，像addtimer（）、deltimer（）和modtimer（）这些函数还是比较简单的，接下来我
们就逐一看一下Go 1.14版本中它们的源码。

1 ．添加

addtimer（）函数的源码如下：

先对t的when和status字段进行校验及修正，然后对pp的timersLock加锁，在锁的保护下调用
cleantimers（）函数清理堆顶，可能存在已被删除的timer，再调用doaddtimer（）函数把t添加到堆
中。添加操作至此就完成了，然后进行解锁操作。最后调用的wakeNetPoller（）函数会根据when
的值按需唤醒阻塞中的netpoller，目的是让调度线程能够及时处理timer。

2 ．删除

deltimer（）函数用来删除一个timer，这里的删除操作主要是修改timer的状态，并不是从堆中移
除，函数的代码如下：

该函数不会改动timer堆，所以不需要对timersLock加锁。正常的处理流程是对处在timerWaiting、
timerModifiedLater和timerModifiedEarlier这 3 种状态的timer用原子操作函数Cas（）先把status改为
timerModifying，再进一步改为timerDeleted，同时要原子性地将相关计数减 1 。调用acquirem（）函
数是为了避免操作过程中被抢占，可能会造成死锁问题。对于timerDeleted、timerRemoving、
timerRemoved和timerNoStatus这几种状态的timer，要么不会再触发，要么根本不在堆中，所以不需
要进行处理。至于timerRunning、timerMoving和timerModifying，分别表示timer正在运行、正在被
移动，以及正在被修改，这种时候不能对timer进行删除操作，必须等到timer脱离当前状态以后再
进一步操作，这 3 种状态都是比较短暂的，所以使用osyield（）函数暂时让出CPU即可。

3 ．修改

modtimer（）函数的代码稍微多一些，我们将代码分成上下两部分来分析。第一部分代码如下：

这个for加switch语句结构跟deltimer（）函数有些类似，凡是以ing结尾的状态都表示需要等待。值
得注意的是wasRemoved用来表示指定的timer已经不在堆中，后面需要与还在堆中的timer分别进行
处理。

第二部分代码如下：

可以看到，对于wasRemoved的timer，需要被添加到堆中，所以对应分支的逻辑与addtimer（）函
数很相似，改动timer堆需要对timersLock加锁。对于原本就在堆中的timer，需要把新的触发时间
when赋值给它的nextwhen字段，而不能直接改动它的when字段，因为在这里不打算改动它在堆里
的位置。新的触发时间如果比原来更早，就把状态设为timerModifiedEarlier，否则状态为
timerModifiedLater。经过这次改动，堆中处于timerModifiedEarlier状态的timer可能增加了一个，也
可能减少了一个，还可能不增不减，如果有变化，就把adjustTimers相应地增加或减少 1 。最后，如

果新的触发时间比原来更早，还要调用wakeNetPoller（）函数，为的是更早地唤醒调度线程，以便
处理timer。

在Go 1.16版中，modtimer（）函数会返回一个bool值，表示被修改的timer是否在运行前完成了修
改，并且会更新p中的timerModifiedEarliest字段。在Go 1.14版中，p中的timer0When用来存储最小
堆堆顶的timer的when字段，表示最早要触发的timer，但是对于修改过的timer，触发时间可能会被
修改成一个更早的时间，却没有相应的字段来记录这个修改后的最早时间。直到Go 1.16版才新增
了这个timerModifiedEarliest字段，用来存储修改后的最早时间。这样一来，在调度器处理timer时，
通过这两个时间中更小的那个就能直接确定最早的触发时间，而不需要对堆进行重排序。

至此，我们已经知道timer是存储在最小堆中的，以及是如何被添加、删除和修改的。接下来看一
下调度器是如何运行timer的。

4 ．运行

先来看两个会被调度器用到的函数，首先是用来调整timer堆的adjusttimers（）函数，代码如下：

如果p.adjustTimers等于 0 ，也就说明没有触发时间比p.timer0When更早的timer，该函数就会直接返
回。因为p.adjustTimers记录的是堆中状态为timerModifiedEarlier的timer的数量，也就是修改后触发
时间被提前的timer的数量。

在接下来的for循环中，会顺便清理掉已删除的timer，因为最小堆的结构特点，删除下标i位置的元
素不会影响前面元素的顺序，所以每次删除后只需将i减 1 ，再继续遍历就不会漏掉内容了。同理，
timerModifiedEarlier和timerModifiedLater两种状态的timer也是先从堆中移除，然后追加到moved切
片中，遍历完成后再由addAdjustedTimers（）函数统一添加回去，这样就可避免中途对整个堆重新
排序，所以只需遍历一次就可以了。addAdjustedTimers（）函数的逻辑很简单，代码如下：

接下来讲解runtimer（）函数，调度器就是通过它来运行timer的，函数的代码如下：

该函数必须在系统栈上运行，for循环中始终取堆顶的那个timer。如果t处于timerWaiting状态，则进
一步比较t.when和当前时间，如果时间还没到就返回t.when，否则就通过runOneTimer（）函数来运
行t，并返回 0 。如果t处于timerDeleted状态，就会通过dodeltimer0（）函数把它从堆中移除，如果
堆的大小变成 0 就返回-1，否则继续循环。如果t处于timerModifiedEarlier或timerModifiedLater状
态，则先把它从堆中移除，然后重新添加进去。整体来看，只要函数的返回值不为 0 ，就表示暂时
没有timer可以运行。

再来看一下runOneTimer（）函数的逻辑，简单起见省略了部分代码，只保留了主要逻辑，代码如
下：

如果t.period字段大于 0 ，也就说明t是个周期性的timer，此时需要把t.when设置为下次触发的时间，
并调整t在堆中的位置，还要按需更新p的timer0When字段。如果是一次性的timer，就将其从堆中移
除。最后，在解锁的情况下调用回调函数f（），完成后重新加锁，这样能够避免因f（）函数中调
用timer相关函数造成死锁的情况。

至此，timer模块的主要函数就梳理得差不多了，接下来看一看调度器是如何处理timer的。还记得
schedule（）函数和findrunnable（）函数都会调用的那个checkTimers（）函数吗？它就是联接调度
循环与timer模块的纽带，函数的代码如下：

函数会先处理p.adjustTimers为 0 的情况，这意味着堆中不存在触发时间被提前的timer，所以
p.timer0When就是最早的触发时间了。p.timer0When＝ 0 ，表示堆是空的，所以不需要进一步处理
了。如果p.timer0When大于当前时间，就表示还没有到达任何timer的触发时间，这时候如果堆中处
于timerDeleted状态的timer数量没有达到总数的1/4，就直接返回。

接下来先对p.timersLock加锁，再通过adjusttimers调整timer堆，这样就能把那些被修改过的timer放
到正确的位置。后续的for循环会一直调用runtimer（）函数，直到timer堆为空或者runtimer（）函
数的返回值不等于 0 。如果runtimer（）函数的返回值大于 0 ，此返回值就是下个timer的触发时间，
作为pollUntil返回，让阻塞式的netpoll能够在适当的时间超时返回。最后的clearDeletedTimers（）
函数保证timer堆能够得到清理，因为adjusttimers（）函数在p.adjustTimers为 0 时不会进行任何操
作，所以这个清理操作是必要的，避免太多已删除的timer影响堆性能。

6.11 netpoller
在Go语言的runtime中，netpoller是负责把IO多路复用和协程调度结合起来的模块。如果goroutine执
行网络IO时需要等待，则netpoller就会自动将其挂起，等到数据就绪以后再将其唤醒，用户代码对
这一切都是无感知的，所以对于开发者来讲非常方便。本节还是从源码入手，分析并探索netpoller
实现的原理。Go语言的源码包含对多平台架构的支持，我们主要研究Linux系统上的netpoller实
现，并且假设大家对epoll已经有了最基本的了解。

6.11.1 跨平台的 netpoller

为了支持多个平台，Go的开发者对netpoller的源码进行了抽象，各个平台共用的逻辑被放置在
netpoll.go文件中，分别适配各个平台的代码都有自己单独的文件，例如netpoll_epoll.go是针对Linux
系统的，netpoll_kqueue.go是针对macOS和BSD系统的。这些适配不同平台的代码被抽象成一组标
准函数，这样一来netpoller的绝大部分代码就不用考虑具体的平台了。在Go 1.14版本中，这组函数
一共有 7 个，函数的原型如下：

接下来就结合netpoll_epoll.go中与Linux系统对应的一组实现，逐个梳理各个函数的用途，源代码摘
选自Go 1.14版本的runtime。

1 ． netpollinit （）函数

netpollinit（）函数用来初始化poller，只会被调用一次。在Linux系统上主要用来创建epoll实例，还
会创建一个非阻塞式pipe，用来唤醒阻塞中的netpoller，代码如下：

其中，epfd、netpollBreakRd和netpollBreakWr都是包级别的变量。efpd是epoll实例的文件描述符，
netpollBreakRd和netpollBreakWr是非阻塞管道两端的文件描述符，分别被用作读取端和写入端。读
取端netpollBreakRd被添加到epoll中监听EPOLLIN事件，后续从写入端netpollBreakWr写入数据就能
唤醒阻塞中的poller。

2 ． netpollIsPollDescriptor （）函数

netpollIsPollDescriptor（）函数用来判断文件描述符fd是否被poller使用，在Linux对应的实现中，只
有epfd、netpollBreakRd和netpollBreakWr属于被poller使用的描述符，函数的代码如下：

3 ． netpollopen （）函数

netpollopen（）函数用来把要监听的文件描述符fd和与之关联的pollDesc结构添加到poller实例中，
在Linux上就是添加到epoll中，代码如下：

文件描述符是以EPOLLET（监听边缘触发模式）被添加到epoll中的，同时监听读、写事件。
pollDesc类型的数据结构pd作为与fd关联的自定义数据会被一同添加到epoll中。

4 ． netpollclose （）函数

netpollclose（）函数用来把文件描述符fd从poller实例中移除，也就是从epoll中删除，代码如下：

5 ． netpollarm （）函数

netpollarm（）函数只有在应用水平触发的系统上才会被用到，Linux不会用到该函数，只是为了通
过编译而用来凑数的，代码如下：

6 ． netpollBreak （）函数

netpollBreak（）函数用来唤醒阻塞中的netpoll，它实际上就是向netpollBreakWr描述符中写入数
据，这样一来epoll就会监听到netpollBreakRd的EPOLLIN事件，代码如下：

因为write调用可能会被打断，所以在遇到EINTR错误的时候，netpollBreak（）函数会通过for循环
持续尝试向netpollBreakWr中写入一字节数据。

7 ． netpoll （）函数

还剩最后一个函数，也是最为关键的，那就是netpoll（）函数。在6.8节分析调度循环的时候，我
们知道该函数会返回一个gList，里面是因为IO数据就绪而能够恢复运行的一组g。我们把函数的源
码分成 3 部分分别进行梳理。

第一部分代码如下：

epfd的初始值是-1，而有效的文件描述符不会小于 0 。epfd仍旧等于-1，表明epoll尚未初始化，此时
netpoll（）函数就会返回一个空的gList。接下来的if语句块把纳秒级的delay转换成了毫秒级的
waitms。

第二部分代码如下：

通过epollwait（）函数等待IO事件，缓冲区大小为 128 个epollevent，超时时间是waitms。如果
epollwait（）函数被中断打断，就通过goto来重试。waitms＞ 0 时不会重试，因为需要返回调用者中
去重新计算超时时间。

第三部分代码如下：

通过for循环遍历所有IO事件。对于文件描述符netpollBreakRd而言，只有EPOLLIN事件是正常的，
其他都会被视为异常。只有在delay不为 0 ，也就是阻塞式netpoll时，才读取netpollBreakRd中的数
据。根据epoll返回的IO事件标志位为mode赋值：r表示可读，w表示可写，r+w表示既可读又可写。
mode不为 0 ，表示有IO事件，需要从ev.data字段得到与IO事件关联的pollDesc，检测IO事件中的错
误标志位，并相应地为pd.everr赋值，最后调用netpollready（）函数。netpollready（）函数的代码
如下：

该函数的作用是，根据mode的值从pollDesc中取出IO需求被满足的goroutine，然后添加到toRun列
表中。例如mode的值是可读或可读可写，而pollDesc中也有等待读事件的goroutine，那么这个
goroutine就该被唤醒继续运行了，所以就会把这个goroutine添加到toRun中。从pollDesc中获得对应
G指针的操作是由netpollunblock（）函数完成的。

在进一步探索之前，需要先弄清楚pollDesc结构中各个字段的含义，每个文件描述符被添加到
netpoller中之后，都由一个pollDesc来表示，该结构的定义代码如下：

通过notinheap注释可以知道，该数据结构不允许被分配在堆上，runtime会使用持久化分配器来为
该结构分配内存，并且实现了专用的pollCache进行缓存。pollDesc各字段的用途如表6-11所示。

表6-11 pollDesc各字段的用途

在了解了pollDesc的结构后，继续看netpollunblock（）函数的代码，代码如下：

首先要讲解的是函数的参数，mode可以是字符r或w，分别表示要取得pd中等待读或等待写的g，
ioready表示与mode相对应的IO事件是否已触发，也就是fd是否可读或可写。

变量gpp，也就是g指针的指针，默认获取的是pd.rg的地址。如果mode是w，则是pd.wg的地址。在
接下来的for循环中，先处理的是old值为pdReady的情况，也就是说IO已经就绪，却没有等待IO的
协程，那么无论本次ioready的值如何，都不需要更新gpp的值，于是直接返回nil。如果old值为 0 ，
并且ioready为false，表示既没有协程在等待，也没有已就绪的IO事件，所以不需要做任何处理，直
接返回nil。接下来声明了变量new，其默认值为 0 ，对应指针类型的nil。如果ioready为真，则new会
被赋值为pdReady。接下来的CAS函数会把新的状态new赋给gpp，并修正old的值。因为old最终会
被强转换为*g类型，所以必须是一个有效的指针或nil。

综上所述，netpollunblock（）函数不会阻塞，它会根据mode和ioready的值从pd中取出等待IO的g，
如果没有，则返回nil。该函数还可能会更新rg或wg的值，新的值为 0 或pdReady。

回过头来看，从netpoll（）函数到netpollready（）函数，再到这里的netpollunblock（）函数，就是
一步步把epollwait（）函数返回的IO事件存储到了对应的pollDesc中。如果有正在等待该事件的协
程，就会被添加到gList中返回，继而被添加到runq中。

至此，我们已经了解了等待IO的协程是如何被netpoller唤醒的，但是协程又是如何因IO等待而挂起
的呢？这可以从标准库中与网络IO相关的函数和方法入手，接下来就以TCP连接的Read方法为入
口，逐层深入分析源码。

6.11.2 TCP 连接的 Read （）方法

net.TCPConn通过嵌入net.conn类型而继承了后者的Read（）方法，而net.（conn）.Read（）方法
会调用net.（netFD）.Read（）方法，后者又会调用internal/poll.（FD）.Read（）方法，后者又会
调用internal/poll.（pollDesc）.waitRead（）方法，waitRead（）方法会调用internal/poll.

（*pollDesc）.wait（）方法。wait（）方法通过调用internal/poll.runtime_pollWait（）函数实现功
能，而后者则是通过linkname机制链接到runtime.poll_runtime_pollWait（）函数，该函数的代码如
下：

该函数最主要的逻辑就是通过netpollblock（）函数实现的，与它的名字一样，netpollblock（）函数
可能会造成调用它的goroutine阻塞而挂起，函数的代码如下：

该函数与netpollunblock（）函数有些相似，不同的是waitio表示是否要挂起以等待IO就绪，返回值
为true，表示IO就绪，false则可能是超时或fd被移除。如果old值为pdReady，就表示当前IO已经处
于就绪状态，所以直接返回true。如果old为 0 ，就先通过CAS把它置为pdWait，表示当前协程即将
挂起等待IO就绪，然后当前协程会调用gopark（）函数来挂起自己，netpollblockcommit（）函数会
把当前g的地址赋值给*gpp。等到挂起的协程被netpoller唤醒后，就会从gopark返回，从gpp中取得
新的IO状态，继续执行后续逻辑。

至此，我们就梳理完了goroutine是如何因为网络IO的原因而被挂起，以及又是如何在IO就绪之后被
netpoller唤醒的。本节关于netpoller的探索就到这里，更多有趣的细节各位读者可自行阅读、分析
源码。

6.12 监控线程
通过6.6节的介绍，我们已经知道监控线程是由main goroutine创建的。监控线程与GMP中的工作线
程不同，并不需要依赖P，也不由GMP模型调度。它会重复执行一系列任务，只不过会视情况调整
自己的休眠时间，接下来我们就简单介绍一下监控线程的主要任务。

6.12.1 按需执行 timer 和 netpoll

在6.10节介绍timer时已经了解到每个P都持有一个最小堆，存储在p.timers中，用于管理自己的
timer，而堆顶的timer就是接下来要触发的那一个，而timer中持有一个回调函数timer.f（），在指
定时间到达后就会调用这个回调函数，但是谁负责在时间到达时调用回调函数呢？

在6.8节介绍调度程序的主要逻辑时，我们知道每次调度时都会调用checkTimers（）函数，检查并
执行已经到时间的那些timer。不过这还不够稳妥，万一所有M都在忙，不能及时触发调度，可能会
导致timer执行时间发生较大的偏差，所以还会通过监控线程来增加一层保障。

当监控线程检测到接下来有timer要执行时，不仅会按需调整休眠时间，还会在没有空闲M时创建新
的工作线程，以保障timer可以顺利执行。

timer有明确的触发时间，但是IO事件的就绪就没那么确定了，所以为了降低IO延迟，需要时不时
地主动轮询，以及时获得就绪的IO事件，也就是执行netpoll。

全局变量sched中会记录上次netpoll执行的时间（sched.lastpoll），如果监控线程检测到距离上次轮
询已超过了10ms，就会再执行一次netpoll。实际上，不只是监控线程，第 6 章介绍过的调度器，以
及第 8 章要介绍的GC在工作过程中都会按需执行netpoll。

6.12.2 抢占 G 和 P
本着公平调度的原则，监控线程会对运行时间过长的G实行抢占操作，也就是告诉那些运行时间超
过特定阈值（10ms）的G，该让出了。

如何确定哪些G运行时间过长了呢？runtime.p中有一个schedtick字段，每当调度执行一个新的G并
且不继承上个G的时间片时，都会让它自增一，相关字段的代码如下：

而p.sysmontick.schedwhen记录的是上一次调度的时间。监控线程如果检测到p.sysmontick.schedtick
与p.schedtick不相等，说明这个P又发生了新的调度，就会同步这里的调度次数，并更新这个调度
时间，相关代码如下：

但是若p.sysmontick.schedtick与p.schedtick相等，就说明自p.sysmontick.schedwhen这个时间点之后，
这个P并未发生新的调度，或者即使发生了新的调度，也继承了之前G的时间片，所以可以通过当
前时间与schedwhen的差值，来判断当前P上的G是否运行时间过长了，代码如下：

如果G真的运行时间过长了，要怎么通知它让出呢？这自然要使用6.9节介绍过的两种抢占方式了，
通过设置stackPreempt标识，或者进行异步抢占。

为了充分利用CPU，监控线程还会抢占处在系统调用中的P。因为一个协程要执行系统调用，就要
切换到g0栈，在系统调用没执行完之前，这个M和这个G不能被分开，但是用不到P，所以在陷入
系统调用之前，当前M会让出P，解除与当前P的强关联，只在m.oldp中记录这个P。P的数目毕竟有
限，如果有其他协程在等待执行，则放任P如此闲置就着实浪费了。还是把它关联到其他M，继续
工作比较划算。

等到当前M从系统调用中恢复后，会先检测之前的P是否被占用，如果没有被占用就继续使用。否
则再去申请一个，如果没申请到，就把当前G放到全局runq中去，然后当前线程就睡眠了。

6.12.3 强制执行 GC
在runtime包的proc.go中有一个init（）函数，它会以forcegchelper（）函数为执行入口创建一个协
程，代码如下：

也就是说在程序初始化时就会创建一个辅助执行GC的协程，只不过它在做完必要的初始化工作后
便会主动让出。等到它恢复执行时，就可以通过gcStart（）函数发起新一轮的GC了，代码如下：

而监控线程会创建gcTriggerTime类型的gcTrigger，这种类型的GC触发器会检测距离上次执行GC的
时间是否已经超过runtime.forcegcperiod，默认为两分钟，代码如下：

如果超过指定时间，同时forcegc还没有被开启，就需修改forcegc的状态信息，并把forcegc.g记录的
协程（程序初始化时创建的那个辅助执行GC的协程）添加到全局runq中。这样等到它得到调度执
行时，就会开启新一轮的GC工作了。

监控线程的主要任务就介绍到这里，保障计时器正常执行，执行网络轮询，抢占长时间运行的或处
在系统调用的P，以及强制执行GC，监控线程的这些工作任务无不是为了保障程序健康高效地执
行。

6.13 本章小结
本章内容较多，稍微有些复杂。开篇先简单分析了进程、线程和协程的不同，实际上就是越来越轻
量。接下来又对比了传统的阻塞式IO、非阻塞式IO，还有近年来流行的IO多路复用，更重要的是
协程和IO多路复用这两项技术的巧妙结合。有了这些铺垫之后，就可以开始深入Go语言的协程调
度了。首先就是GMP模型，从基本概念到主要的数据结构，然后结合源码分析，逐步梳理了调度
器的初始化、协程的创建与退出，还有最核心的调度循环。之后用一个实例，通过调试加源码分析
的方式，深入对比了Go 1.13版本和Go 1.14版本中抢占式调度的不同实现，笔者认为Go 1.14版本以
后才是真正的抢占。最后几节主要基于源码分析，梳理了timer、netpoller的实现细节，以及监控线
程的主要工作。虽然整体有些繁杂，但是对于想要深入了解goroutine的读者，还是有一定的参考价
值的。
